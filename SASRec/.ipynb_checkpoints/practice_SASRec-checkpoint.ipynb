{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T06:07:29.538194Z",
     "start_time": "2023-11-11T06:07:29.057989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                2.1.0\r\n",
      "torch_geometric      2.4.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuXWJLEm2UWS"
   },
   "source": [
    "# **AAI0026 Practice : SASRec**\n",
    "## **Self-Attentive Sequential Recommendation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGKqVEbbMEzf"
   },
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T06:08:45.726677Z",
     "start_time": "2023-11-11T06:08:43.194420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0NiFL6OLpaJ"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T14:38:26.320274Z",
     "start_time": "2023-10-31T14:38:25.487996Z"
    },
    "id": "By2oyBw7Lrh5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Process, Queue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nwwq0nSdmsOL"
   },
   "source": [
    "# 1 Input Action Sequence\n",
    "- Load MovieLens Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sf7vUmdNKCjA"
   },
   "source": [
    "MovieLens has several versions available. We will use the MovieLens 1M dataset, which has 1 million ratings from 6000 users on 4000 movies. Each line of the dataset file consists of a user ID, a movie ID, the user's rating on the movie, and a timestamp. However, given that SASRec is designed to learn from implicit information (interaction between the user and the movie), we will only utilize user IDs and movie IDs in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmKQPpUaYFXc"
   },
   "source": [
    "## Train/Validation/Test Data Generation\n",
    "\n",
    "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=1G-S7FpUzlykFBNmVh-CtLUpmHh0-Y5XG\">\n",
    "</p>\n",
    "\n",
    "We will begin by loading the dataset into a Python dictionary with User ID = Key and Movie ID = Value. Then we will split the dataset to training/validation/test data. The second-last and the last Movie IDs from each user will be assigned for the validation and test data respectively while all the remaining will constitute the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T14:40:08.605766Z",
     "start_time": "2023-10-31T14:40:08.590743Z"
    },
    "id": "vCmgBy-2VgJs"
   },
   "outputs": [],
   "source": [
    "def data_partition(data_path):\n",
    "   # TODO: Implement this function that takes a text file,\n",
    "   # convert the text file to the dictionary,\n",
    "   # and then split the dictionary to the training, validation, and test data\n",
    "\n",
    "    usernum = 0\n",
    "    itemnum = 0\n",
    "    User = defaultdict(list) # create a dictionary with default value as empty list\n",
    "    user_train = {}\n",
    "    user_valid = {}\n",
    "    user_test = {}\n",
    "\n",
    "    os.system(f'wget {data_path} -O data.txt')\n",
    "    with open('data.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            u, i = map(int, line.rstrip().split(' '))\n",
    "            User[u].append(i)\n",
    "\n",
    "      ############# Your code here ############\n",
    "      ## Note:\n",
    "      ## 1. Compute the number 'usernum' of users, and\n",
    "      ##    the number 'itemnum' of items from the dictionary 'User'\n",
    "      ## 2. Each line of 'data.txt' consists of a user ID and an item ID\n",
    "      ## 3. A user ID starts from 1. User IDs are sorted in ascending order\n",
    "      ## 4. Hint:use max() function\n",
    "\n",
    "        usernum = max(User.keys())\n",
    "        itemnum = max(max(User.values()))\n",
    "\n",
    "      #########################################\n",
    "\n",
    "    # The second-last and the last movies are for validation and test data respectively\n",
    "        for user in User:\n",
    "            nfeedback = len(User[user])\n",
    "            if nfeedback < 3:\n",
    "                user_train[user] = User[user]\n",
    "                user_valid[user] = []\n",
    "                user_test[user] = []\n",
    "            else:\n",
    "                user_train[user] = User[user][:-2]\n",
    "                user_valid[user] = []\n",
    "                user_valid[user].append(User[user][-2])\n",
    "                user_test[user] = []\n",
    "                user_test[user].append(User[user][-1])\n",
    "    return [user_train, user_valid, user_test, usernum, itemnum]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sCV3xJWCddX"
   },
   "source": [
    "## Question 1: What is the number of Users and Items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T14:40:22.385337Z",
     "start_time": "2023-10-31T14:40:17.367093Z"
    },
    "id": "1Np-FxA6tG1J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-10-31 23:40:18--  https://drive.google.com/uc?id=1sjeWz4pXkVGmy__Tr8zCtGV6rPGZfgLy\n",
      "Resolving drive.google.com (drive.google.com)... 142.250.76.142\n",
      "Connecting to drive.google.com (drive.google.com)|142.250.76.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-08-1g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hfnh2gfud81c9amj22vff10b7i7gthq4/1698763200000/01838841618913860328/*/1sjeWz4pXkVGmy__Tr8zCtGV6rPGZfgLy?uuid=3114f099-3e26-4d69-95d8-436045ca00e6 [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2023-10-31 23:40:19--  https://doc-08-1g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hfnh2gfud81c9amj22vff10b7i7gthq4/1698763200000/01838841618913860328/*/1sjeWz4pXkVGmy__Tr8zCtGV6rPGZfgLy?uuid=3114f099-3e26-4d69-95d8-436045ca00e6\n",
      "Resolving doc-08-1g-docs.googleusercontent.com (doc-08-1g-docs.googleusercontent.com)... 142.250.207.97\n",
      "Connecting to doc-08-1g-docs.googleusercontent.com (doc-08-1g-docs.googleusercontent.com)|142.250.207.97|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9053831 (8.6M) [text/plain]\n",
      "Saving to: `data.txt'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0%  666K 13s\n",
      "    50K .......... .......... .......... .......... ..........  1%  955K 11s\n",
      "   100K .......... .......... .......... .......... ..........  1% 2.95M 8s\n",
      "   150K .......... .......... .......... .......... ..........  2% 1.58M 8s\n",
      "   200K .......... .......... .......... .......... ..........  2% 3.08M 7s\n",
      "   250K .......... .......... .......... .......... ..........  3% 4.95M 6s\n",
      "   300K .......... .......... .......... .......... ..........  3% 3.69M 5s\n",
      "   350K .......... .......... .......... .......... ..........  4% 8.42M 5s\n",
      "   400K .......... .......... .......... .......... ..........  5% 5.45M 4s\n",
      "   450K .......... .......... .......... .......... ..........  5% 9.93M 4s\n",
      "   500K .......... .......... .......... .......... ..........  6% 5.81M 4s\n",
      "   550K .......... .......... .......... .......... ..........  6% 6.20M 3s\n",
      "   600K .......... .......... .......... .......... ..........  7% 12.4M 3s\n",
      "   650K .......... .......... .......... .......... ..........  7% 14.1M 3s\n",
      "   700K .......... .......... .......... .......... ..........  8% 12.7M 3s\n",
      "   750K .......... .......... .......... .......... ..........  9% 6.46M 3s\n",
      "   800K .......... .......... .......... .......... ..........  9% 32.9M 3s\n",
      "   850K .......... .......... .......... .......... .......... 10% 13.4M 2s\n",
      "   900K .......... .......... .......... .......... .......... 10% 24.3M 2s\n",
      "   950K .......... .......... .......... .......... .......... 11% 9.91M 2s\n",
      "  1000K .......... .......... .......... .......... .......... 11% 27.4M 2s\n",
      "  1050K .......... .......... .......... .......... .......... 12% 11.4M 2s\n",
      "  1100K .......... .......... .......... .......... .......... 13% 15.8M 2s\n",
      "  1150K .......... .......... .......... .......... .......... 13% 17.4M 2s\n",
      "  1200K .......... .......... .......... .......... .......... 14% 22.2M 2s\n",
      "  1250K .......... .......... .......... .......... .......... 14% 11.6M 2s\n",
      "  1300K .......... .......... .......... .......... .......... 15% 30.2M 2s\n",
      "  1350K .......... .......... .......... .......... .......... 15% 61.5M 2s\n",
      "  1400K .......... .......... .......... .......... .......... 16% 15.1M 2s\n",
      "  1450K .......... .......... .......... .......... .......... 16% 19.2M 2s\n",
      "  1500K .......... .......... .......... .......... .......... 17% 66.3M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 18% 21.4M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 18% 48.8M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 19% 45.6M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 19% 27.4M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 20% 14.7M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 20% 1.57M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 21% 22.0M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 22%  164M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 22% 49.4M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 23% 75.5M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 23%  126M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 24% 43.2M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 24% 52.6M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 25% 29.4M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 26% 24.1M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 26% 56.3M 1s\n",
      "  2350K .......... .......... .......... .......... .......... 27%  156M 1s\n",
      "  2400K .......... .......... .......... .......... .......... 27% 38.5M 1s\n",
      "  2450K .......... .......... .......... .......... .......... 28% 95.4M 1s\n",
      "  2500K .......... .......... .......... .......... .......... 28% 55.2M 1s\n",
      "  2550K .......... .......... .......... .......... .......... 29% 64.9M 1s\n",
      "  2600K .......... .......... .......... .......... .......... 29% 34.2M 1s\n",
      "  2650K .......... .......... .......... .......... .......... 30% 64.7M 1s\n",
      "  2700K .......... .......... .......... .......... .......... 31% 87.2M 1s\n",
      "  2750K .......... .......... .......... .......... .......... 31% 39.8M 1s\n",
      "  2800K .......... .......... .......... .......... .......... 32%  136M 1s\n",
      "  2850K .......... .......... .......... .......... .......... 32% 38.4M 1s\n",
      "  2900K .......... .......... .......... .......... .......... 33%  118M 1s\n",
      "  2950K .......... .......... .......... .......... .......... 33%  124M 1s\n",
      "  3000K .......... .......... .......... .......... .......... 34% 91.3M 1s\n",
      "  3050K .......... .......... .......... .......... .......... 35% 23.8M 1s\n",
      "  3100K .......... .......... .......... .......... .......... 35%  127M 1s\n",
      "  3150K .......... .......... .......... .......... .......... 36% 37.7M 1s\n",
      "  3200K .......... .......... .......... .......... .......... 36% 78.3M 1s\n",
      "  3250K .......... .......... .......... .......... .......... 37% 24.2M 1s\n",
      "  3300K .......... .......... .......... .......... .......... 37% 11.1M 1s\n",
      "  3350K .......... .......... .......... .......... .......... 38%  120M 1s\n",
      "  3400K .......... .......... .......... .......... .......... 39%  135M 1s\n",
      "  3450K .......... .......... .......... .......... .......... 39%  131M 1s\n",
      "  3500K .......... .......... .......... .......... .......... 40%  184M 1s\n",
      "  3550K .......... .......... .......... .......... .......... 40%  182M 1s\n",
      "  3600K .......... .......... .......... .......... .......... 41% 4.75M 1s\n",
      "  3650K .......... .......... .......... .......... .......... 41%  100M 1s\n",
      "  3700K .......... .......... .......... .......... .......... 42% 42.8M 1s\n",
      "  3750K .......... .......... .......... .......... .......... 42% 93.0M 1s\n",
      "  3800K .......... .......... .......... .......... .......... 43% 36.9M 1s\n",
      "  3850K .......... .......... .......... .......... .......... 44% 84.6M 1s\n",
      "  3900K .......... .......... .......... .......... .......... 44% 55.0M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 45% 5.20M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 45% 55.9M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 46%  761K 1s\n",
      "  4100K .......... .......... .......... .......... .......... 46% 38.6M 1s\n",
      "  4150K .......... .......... .......... .......... .......... 47%  133M 1s\n",
      "  4200K .......... .......... .......... .......... .......... 48% 79.3M 1s\n",
      "  4250K .......... .......... .......... .......... .......... 48% 54.4M 1s\n",
      "  4300K .......... .......... .......... .......... .......... 49% 56.5M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 49% 88.1M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 50% 22.3M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 50%  107M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 51%  103M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 52% 59.6M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 52% 55.9M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 53% 64.0M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 53% 36.6M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 54%  150M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 54%  462K 0s\n",
      "  4850K .......... .......... .......... .......... .......... 55% 90.6M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 55%  215M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 56%  237M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 57%  188M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 57%  204M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 58%  205M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 58%  190M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 59%  179M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 59%  191M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 60%  218M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 61%  155M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 61%  180M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 62%  179M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 62%  150M 0s\n",
      "  5550K .......... .......... .......... .......... .......... 63%  170M 0s\n",
      "  5600K .......... .......... .......... .......... .......... 63%  159M 0s\n",
      "  5650K .......... .......... .......... .......... .......... 64%  202M 0s\n",
      "  5700K .......... .......... .......... .......... .......... 65%  198M 0s\n",
      "  5750K .......... .......... .......... .......... .......... 65%  201M 0s\n",
      "  5800K .......... .......... .......... .......... .......... 66%  162M 0s\n",
      "  5850K .......... .......... .......... .......... .......... 66%  133M 0s\n",
      "  5900K .......... .......... .......... .......... .......... 67%  208M 0s\n",
      "  5950K .......... .......... .......... .......... .......... 67%  233M 0s\n",
      "  6000K .......... .......... .......... .......... .......... 68%  166M 0s\n",
      "  6050K .......... .......... .......... .......... .......... 68%  210M 0s\n",
      "  6100K .......... .......... .......... .......... .......... 69%  225M 0s\n",
      "  6150K .......... .......... .......... .......... .......... 70%  192M 0s\n",
      "  6200K .......... .......... .......... .......... .......... 70%  208M 0s\n",
      "  6250K .......... .......... .......... .......... .......... 71%  207M 0s\n",
      "  6300K .......... .......... .......... .......... .......... 71%  226M 0s\n",
      "  6350K .......... .......... .......... .......... .......... 72%  206M 0s\n",
      "  6400K .......... .......... .......... .......... .......... 72%  195M 0s\n",
      "  6450K .......... .......... .......... .......... .......... 73%  233M 0s\n",
      "  6500K .......... .......... .......... .......... .......... 74%  233M 0s\n",
      "  6550K .......... .......... .......... .......... .......... 74%  230M 0s\n",
      "  6600K .......... .......... .......... .......... .......... 75%  201M 0s\n",
      "  6650K .......... .......... .......... .......... .......... 75%  236M 0s\n",
      "  6700K .......... .......... .......... .......... .......... 76%  242M 0s\n",
      "  6750K .......... .......... .......... .......... .......... 76%  242M 0s\n",
      "  6800K .......... .......... .......... .......... .......... 77%  181M 0s\n",
      "  6850K .......... .......... .......... .......... .......... 78%  153M 0s\n",
      "  6900K .......... .......... .......... .......... .......... 78%  201M 0s\n",
      "  6950K .......... .......... .......... .......... .......... 79%  202M 0s\n",
      "  7000K .......... .......... .......... .......... .......... 79%  207M 0s\n",
      "  7050K .......... .......... .......... .......... .......... 80%  197M 0s\n",
      "  7100K .......... .......... .......... .......... .......... 80%  158M 0s\n",
      "  7150K .......... .......... .......... .......... .......... 81%  108M 0s\n",
      "  7200K .......... .......... .......... .......... .......... 81%  106M 0s\n",
      "  7250K .......... .......... .......... .......... .......... 82%  126M 0s\n",
      "  7300K .......... .......... .......... .......... .......... 83%  126M 0s\n",
      "  7350K .......... .......... .......... .......... .......... 83%  132M 0s\n",
      "  7400K .......... .......... .......... .......... .......... 84%  111M 0s\n",
      "  7450K .......... .......... .......... .......... .......... 84%  132M 0s\n",
      "  7500K .......... .......... .......... .......... .......... 85%  125M 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7550K .......... .......... .......... .......... .......... 85%  133M 0s\n",
      "  7600K .......... .......... .......... .......... .......... 86%  158M 0s\n",
      "  7650K .......... .......... .......... .......... .......... 87%  186M 0s\n",
      "  7700K .......... .......... .......... .......... .......... 87%  187M 0s\n",
      "  7750K .......... .......... .......... .......... .......... 88%  184M 0s\n",
      "  7800K .......... .......... .......... .......... .......... 88%  127M 0s\n",
      "  7850K .......... .......... .......... .......... .......... 89%  173M 0s\n",
      "  7900K .......... .......... .......... .......... .......... 89%  104M 0s\n",
      "  7950K .......... .......... .......... .......... .......... 90%  187M 0s\n",
      "  8000K .......... .......... .......... .......... .......... 91%  133M 0s\n",
      "  8050K .......... .......... .......... .......... .......... 91%  151M 0s\n",
      "  8100K .......... .......... .......... .......... .......... 92%  155M 0s\n",
      "  8150K .......... .......... .......... .......... .......... 92%  311K 0s\n",
      "  8200K .......... .......... .......... .......... .......... 93% 13.4M 0s\n",
      "  8250K .......... .......... .......... .......... .......... 93%  103M 0s\n",
      "  8300K .......... .......... .......... .......... .......... 94% 83.9M 0s\n",
      "  8350K .......... .......... .......... .......... .......... 95% 39.8M 0s\n",
      "  8400K .......... .......... .......... .......... .......... 95% 21.7M 0s\n",
      "  8450K .......... .......... .......... .......... .......... 96% 39.5M 0s\n",
      "  8500K .......... .......... .......... .......... .......... 96% 27.3M 0s\n",
      "  8550K .......... .......... .......... .......... .......... 97%  126M 0s\n",
      "  8600K .......... .......... .......... .......... .......... 97% 28.7M 0s\n",
      "  8650K .......... .......... .......... .......... .......... 98% 74.9M 0s\n",
      "  8700K .......... .......... .......... .......... .......... 98% 43.7M 0s\n",
      "  8750K .......... .......... .......... .......... .......... 99% 32.2M 0s\n",
      "  8800K .......... .......... .......... .......... .         100%  107M=0.8s\n",
      "\n",
      "2023-10-31 23:40:21 (10.9 MB/s) - `data.txt' saved [9053831/9053831]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users: 6040, number of items: 3412\n"
     ]
    }
   ],
   "source": [
    "data_path = 'https://drive.google.com/uc?id=1sjeWz4pXkVGmy__Tr8zCtGV6rPGZfgLy'\n",
    "user_train, user_valid, user_test, usernum, itemnum = data_partition(data_path)\n",
    "\n",
    "print(f'number of users: {usernum}, number of items: {itemnum}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqOcxa0vtFEA"
   },
   "source": [
    "## Question 2: What is the ID of the movie that User 26 has watched most recently?\n",
    "### (use dictionary user_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T14:40:38.477010Z",
     "start_time": "2023-10-31T14:40:38.471855Z"
    },
    "id": "ATGeg8jWt8VP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last movie that User 26 has watched: [901]\n"
     ]
    }
   ],
   "source": [
    "print(f'The last movie that User 26 has watched: {user_test[26]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NHYKpHeYJHm"
   },
   "source": [
    "## Sampler for Batch Generation\n",
    "\n",
    "\n",
    "<img width=\"800\" alt=\"1\" src=\"https://drive.google.com/uc?id=1cbB_RfDtPN3kGOcO_FDEz3YbXt6y1a39\">\n",
    "</p>\n",
    "\n",
    "As each user has interacted with a different number of movies, we should transform a training sequence into the fixed-length sequence with the maximum length (maxlen=200). If the sequence length is greater than 200, we retain only the most recent 200 movies. If the sequence length is less than 200, we repeatedly add a padding item (0) to the left until the length becomes 200. A constant zero vector is used as the embedding for the padding item.\n",
    "\n",
    "- seq: Input NumPy ndarray (batch_size*maxlen) for Embedding Layer.\n",
    "- pos: Ground Truth NumPy ndarray (batch_size*maxlen), the first movie is deleted and the last movie is added as our task is to predict the next item.\n",
    "- neg: Negative Movie Lists Numpy ndarray (batch_size*maxlen), randomly sampled from the movies that have interacted with no users.\n",
    "\n",
    "NumPy ndarrays seg, pos, and neg have the same shape, and pos and neg will be used when computing loss at Prediction Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T14:48:32.662281Z",
     "start_time": "2023-10-31T14:48:32.639206Z"
    },
    "id": "F_vhRYIXYbmy"
   },
   "outputs": [],
   "source": [
    "#Negative Sampling: select items not in the set of positive items\n",
    "def random_neq(l, r, s):\n",
    "    ##TODO: Implement this function that takes two indices 'l', 'r', and set 's',\n",
    "    ##and return a random integer number t in the range [l,r]\n",
    "    ##but not in set 's' so it can generate a new random number\n",
    "\n",
    "    t = np.random.randint(l, r)\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. to ensure that t is not in set 's', use while loop\n",
    "    ## 2. repeat generating random numbers until t is not in set 's'\n",
    "    ## 3. (~3 lines of code)\n",
    "    while t in s:\n",
    "        t = np.random.randint(l, r)\n",
    "    return t\n",
    "\n",
    "    #########################################\n",
    "\n",
    "#generate seg, pos, neg numpy ndarrays.\n",
    "#they are the fixed-length sequences ('maxlen')\n",
    "\n",
    "def sample_function(user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED):\n",
    "    def sample():\n",
    "    ##TODO: Implement this function that takes arguments,\n",
    "    ##and return numpy ndarrays user, seq, pos and neg\n",
    "\n",
    "        user = np.random.randint(1, usernum + 1)\n",
    "        while len(user_train[user]) <= 1: user = np.random.randint(1, usernum + 1)\n",
    "\n",
    "        seq = np.zeros([maxlen], dtype=np.int32) #zeros for padding\n",
    "        pos = np.zeros([maxlen], dtype=np.int32)\n",
    "        neg = np.zeros([maxlen], dtype=np.int32)\n",
    "        nxt = user_train[user][-1]\n",
    "        idx = maxlen - 1\n",
    "\n",
    "        ts = set(user_train[user]) #positive item set\n",
    "        for i in reversed(user_train[user][:-1]): #reverse for padding from the left\n",
    "\n",
    "            seq[idx] = i\n",
    "            pos[idx] = nxt\n",
    "\n",
    "            if nxt != 0:\n",
    "            ############# Your code here ############\n",
    "            ##Note:\n",
    "            ##1. generate neg Numpy ndarray by using random_neg(l,r,s)\n",
    "            ##2. fill in neg[idx] by using the for loop we are in\n",
    "            ##3. remember item ID starts from 1\n",
    "            ##4. (~1 line of code)\n",
    "                neg_item = random_neq(1, itemnum + 1, ts)\n",
    "                neg[idx] = neg_item\n",
    "\n",
    "\n",
    "            #########################################\n",
    "            nxt = i\n",
    "            idx -= 1\n",
    "            if idx == -1: break\n",
    "\n",
    "        return (user, seq, pos, neg)\n",
    "\n",
    "    # Make sure random numbers generated are always the same everytime\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    while True:\n",
    "        one_batch = []\n",
    "        for i in range(batch_size):\n",
    "            one_batch.append(sample())\n",
    "        result_queue.put(zip(*one_batch))\n",
    "\n",
    "class WarpSampler(object):\n",
    "    def __init__(self, User, usernum, itemnum, batch_size=64, maxlen=10, n_workers=1):\n",
    "    ##TODO: Implement this function that takes arguments,\n",
    "    ##and process sampling in parallel using multiple worker processes\n",
    "\n",
    "        self.result_queue = Queue(maxsize=n_workers*10)\n",
    "        self.processors = []\n",
    "\n",
    "        for i in range(n_workers):\n",
    "\n",
    "            ############# Your code here ###################################\n",
    "            ##Note:\n",
    "            ##1. append \"Process(target, args)\" object in self.processors\n",
    "            ##2. set target to \"sample_function\"\n",
    "            ##3. create a tuple of all arguemnts of sample_function,\n",
    "            ##.  result_queue='self.result_queue', and SEED='np.random.randint(2e9)'.\n",
    "            ##4. set args to this tuple\n",
    "#             def sample_function(user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED):\n",
    "#             sampler = WarpSampler(user_train, usernum, itemnum, batch_size=64, maxlen=10, n_workers=3)\n",
    "\n",
    "            self.processors.append(\n",
    "                Process(target=sample_function, args=(User,usernum,itemnum,batch_size,maxlen,self.result_queue,np.random.randint(2e9)))\n",
    "            )\n",
    "            #################################################################\n",
    "\n",
    "            self.processors[-1].daemon = True #doesn't prevent the program from exiting\n",
    "            self.processors[-1].start() #To begine the Multiprocess\n",
    "\n",
    "    #get result_queue when it becomes available, without waiting for all processes to finish\n",
    "    def next_batch(self):\n",
    "        return self.result_queue.get()\n",
    "\n",
    "    def close(self):\n",
    "        for p in self.processors:\n",
    "            p.terminate()\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKhcVeAhCwoY"
   },
   "source": [
    "## Question 3: Does our sampler work well for User 26?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T15:00:33.169221Z",
     "start_time": "2023-10-31T15:00:29.348934Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1696494444898,
     "user": {
      "displayName": "Jane HAN",
      "userId": "01838841618913860328"
     },
     "user_tz": -540
    },
    "id": "jPWl0hDO341V",
    "outputId": "60b00d9f-9316-4fa3-98c1-8b17f61d7a58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/kvsco/.pyenv/versions/3.9.1/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/kvsco/.pyenv/versions/3.9.1/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'sample_function' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sampler \u001b[38;5;241m=\u001b[39m \u001b[43mWarpSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musernum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitemnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# Check one batch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     u, seq, pos, neg \u001b[38;5;241m=\u001b[39m sampler\u001b[38;5;241m.\u001b[39mnext_batch()\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mWarpSampler.__init__\u001b[0;34m(self, User, usernum, itemnum, batch_size, maxlen, n_workers)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m#################################################################\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m#doesn't prevent the program from exiting\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampler = WarpSampler(user_train, usernum, itemnum, batch_size=64, maxlen=10, n_workers=3)\n",
    "\n",
    "for _ in range(1):  # Check one batch\n",
    "    u, seq, pos, neg = sampler.next_batch()\n",
    "\n",
    "    ############# Your code here ###################################\n",
    "    ##note:\n",
    "    ##1.use any() function\n",
    "    ##2.check if neg[26] items are in pos[26]\n",
    "\n",
    "    same_item =any(neg[26] == item for item in pos[26])\n",
    "\n",
    "    #################################################################\n",
    "    if same_item==False:\n",
    "        print(\"Works Well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2Uhrl6Mqsp2"
   },
   "source": [
    "# 2 SASRec Model\n",
    "\n",
    "Now we will implement our SASRec model!\n",
    "\n",
    "Please see the following:\n",
    "\n",
    "1.   Embedding Layer (Item Embedding, Position Embedding):\n",
    "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=16w1sfleDGnnKEQvL-wwPO4REwt3te1TA\">\n",
    "</p>\n",
    "\n",
    "  We will obtain an `item embedding` and a learnable `position embedding` by using this layer. Every item ID in `seq` turns into its item embedding, which gives Tensor(M) of `item embeddings` in shape batch_size(128) x max_len(200) x d(50). Since the self-attention model doesn't include any recurrent or convolutional module, it's impossible to be aware of the positions of previous items. Hence we inject a `position embedding` that represents the position of value in `seq`, i.e., max_len (200) positions ranging 0~199. The positions are encoded to Tensor(P) with the same shape as M. Final `input embedding(E=M+P)` serves as Key and Value, and the layer normalization of input embedding serves as Query.\n",
    "\n",
    "2.   Self-Attention Layer:\n",
    "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=1UuTRCQuCQjf3p49wxPO4z3_95xGE0SqR\">\n",
    "</p>\n",
    "\n",
    "     - Feed (Q,K,V) into Attention layer and compute `attention scores(S)`\n",
    "     - when proceeding dot production of Query and Key (Q*K), need `Attention Masking` to avoid cheeting\n",
    "     - Residual Connection\n",
    "     - Layer Normalization\n",
    "\n",
    "3.   Pointwise Feed Forward Network:\n",
    "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=1bij9ohJtxj4UhpB9OcjRnK8rw9bQLqVz\">\n",
    "</p>\n",
    "\n",
    "  Feed Forward Network consists of \"two 1d Convolution Layers\" which functions as linear transformation. Before passing 1d-Convolution Layer, input data need to be transposed to allow the kernel to move across time-steps. These Convolution Layers are linked with `ReLU activation` and their outputs will be added with the input data for Residual Connection. After Forward Layer the masking (timeline mask), which was used in item embedding, is applied to preserve their initial padding positions.    \n",
    "\n",
    "4.   Prediction Layer:\n",
    "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=14ACxSsuyxeKdtA8QG2HN_XGzV0-p_xSc\">\n",
    "</p>\n",
    "\n",
    "  Finally, Prediction Layer generates `pos_logits` and `neg_logits`, i.e., the scores of movies in positive sets and negative sets respectively for users.\n",
    "  We compute the \"element-wise product\" between pos embedding and attention scores (`log_feats`) obtained in Attention Layer to generate `pos_logits`. We also compute the element-wise product between neg_embedding and the attention score to generate `neg_logits`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQqKUwqU5Qf8"
   },
   "source": [
    "## (1) Build Layers for SASRec\n",
    "- Embedding Layer\n",
    "- Self-Attention Layer\n",
    "- Forward Layer\n",
    "- Prediction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEsLOTQkpsUC"
   },
   "outputs": [],
   "source": [
    "class SASRec(torch.nn.Module):\n",
    "    def __init__(self, user_num, item_num, args):\n",
    "        super(SASRec, self).__init__()\n",
    "\n",
    "        ##TODO: stack embedding layer, attention layer and forward layer,\n",
    "        ##define the function that computes Attention Scores(log_feat)\n",
    "        ##and get pos_logits, neg_logits from FFN and Prediction\n",
    "\n",
    "        self.user_num = user_num\n",
    "        self.item_num = item_num\n",
    "        self.dev = args['device']\n",
    "\n",
    "        self.item_emb = torch.nn.Embedding(self.item_num+1, args['hidden_units'], padding_idx=0)\n",
    "\n",
    "        ##################Your code here ##################\n",
    "        ## Note:\n",
    "        ## 1. Define position embedding and dropout\n",
    "        ## 2. See how item embedding is created above\n",
    "        ## 3. use arg['maxlen'], args['hidden_units'], args['dropout_rate']\n",
    "        ## 4. position embedding doesn't need padding idx\n",
    "\n",
    "        self.pos_emb =\n",
    "        self.emb_dropout =\n",
    "\n",
    "        ###################################################\n",
    "\n",
    "        self.attention_layernorms = torch.nn.ModuleList() # to be Query for self-attention\n",
    "        self.attention_layers = torch.nn.ModuleList() #multi-head for self-attention\n",
    "        self.forward_layernorms = torch.nn.ModuleList()\n",
    "        self.forward_layers = torch.nn.ModuleList()\n",
    "        self.last_layernorm = torch.nn.LayerNorm(args['hidden_units'], eps=1e-8)\n",
    "\n",
    "        for _ in range(args['num_blocks']): #stacks 2 blocks\n",
    "\n",
    "            new_attn_layernorm = torch.nn.LayerNorm(args['hidden_units'], eps=1e-8)\n",
    "            self.attention_layernorms.append(new_attn_layernorm)\n",
    "\n",
    "            new_attn_layer =  torch.nn.MultiheadAttention(args['hidden_units'],\n",
    "                                                            args['num_heads'],\n",
    "                                                            args['dropout_rate'])\n",
    "            self.attention_layers.append(new_attn_layer)\n",
    "\n",
    "            new_fwd_layernorm = torch.nn.LayerNorm(args['hidden_units'], eps=1e-8)\n",
    "            self.forward_layernorms.append(new_fwd_layernorm)\n",
    "\n",
    "            new_fwd_layer = PointWiseFeedForward(args['hidden_units'], args['dropout_rate'])\n",
    "            self.forward_layers.append(new_fwd_layer)\n",
    "\n",
    "            self.pos_sigmoid = torch.nn.Sigmoid()\n",
    "            self.neg_sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def log2feats(self, log_seqs):\n",
    "        ## TODO: Implement this function that takes log_seqs,\n",
    "        ## and make input tensor(item embedding + position embedding) for self-attention layer\n",
    "        ## then return attention scores(log_feats) tensor from self-attention layer\n",
    "\n",
    "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.dev))\n",
    "        seqs *= self.item_emb.embedding_dim ** 0.5 #scaling to stabilize the training process\n",
    "\n",
    "        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1])\n",
    "\n",
    "        ##################Your code here ##################\n",
    "        ## Note:\n",
    "        ## 1. Sum position embedding to input embedding(seqs)\n",
    "        ## 2. Proceed the dropout after\n",
    "        ## 3. (~2 lines of code)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ###################################################\n",
    "\n",
    "        timeline_mask = torch.BoolTensor(log_seqs == 0).to(self.dev)\n",
    "        seqs *= ~timeline_mask.unsqueeze(-1) # broadcast in last dim\n",
    "\n",
    "        tl = seqs.shape[1] # time dim len for enforce causality\n",
    "        attention_mask = ~torch.tril(torch.ones((tl, tl), dtype=torch.bool, device=self.dev))\n",
    "        #to mask upper triangular part\n",
    "\n",
    "        for i in range(len(self.attention_layers)):\n",
    "            seqs = torch.transpose(seqs, 0, 1)\n",
    "\n",
    "           ##################Your code here ##################\n",
    "           ## Note:\n",
    "           ## 1. get Q by attention_layernorms[]()\n",
    "           ## 2. get multihead attention outputs (mha_outputs): sum of the weighted V\n",
    "           ##    by attention_layers[]() with using attention_mask\n",
    "           ## 3. key, value = seqs\n",
    "\n",
    "            Q =\n",
    "            mha_outputs, _ =\n",
    "\n",
    "            ###################################################\n",
    "            seqs = Q + mha_outputs\n",
    "            seqs = torch.transpose(seqs, 0, 1)\n",
    "\n",
    "            seqs = self.forward_layernorms[i](seqs)\n",
    "            seqs = self.forward_layers[i](seqs)\n",
    "            seqs *=  ~timeline_mask.unsqueeze(-1)\n",
    "\n",
    "        log_feats = self.last_layernorm(seqs)\n",
    "\n",
    "        return log_feats #Attention Scores\n",
    "\n",
    "    def forward(self, user_ids, log_seqs, pos_seqs, neg_seqs):\n",
    "        log_feats = self.log2feats(log_seqs)\n",
    "\n",
    "        pos_embs = self.item_emb(torch.LongTensor(pos_seqs).to(self.dev))\n",
    "        neg_embs = self.item_emb(torch.LongTensor(neg_seqs).to(self.dev))\n",
    "\n",
    "        ##################Your code here ##################\n",
    "        ## Note: get pos_logits and neg_logits\n",
    "        ## 1.compute Attention Scores * Value(pos_embs/neg_embs)\n",
    "        ## Hint: use sum(dim=-1)\n",
    "        ## (~2 lines of code)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ###################################################\n",
    "\n",
    "        pos_pred = self.pos_sigmoid(pos_logits)\n",
    "        neg_pred = self.neg_sigmoid(neg_logits)\n",
    "\n",
    "        return pos_logits, neg_logits\n",
    "\n",
    "    def predict(self, user_ids, log_seqs, item_indices):\n",
    "        log_feats = self.log2feats(log_seqs)\n",
    "\n",
    "        final_feat = log_feats[:, -1, :] # the final Attention Scores for the prediction\n",
    "\n",
    "        item_embs = self.item_emb(torch.LongTensor(item_indices).to(self.dev))\n",
    "\n",
    "        logits = item_embs.matmul(final_feat.unsqueeze(-1)).squeeze(-1) #dot product between item's embedding and final feature\n",
    "                                                                        #squeeze to make dimensions match\n",
    "        preds = self.pos_sigmoid(logits)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUu28G0ihX2J"
   },
   "source": [
    "## Question 4: What does the output (preds) of SASRec mean?\n",
    "\n",
    "(1) list of next items to recommend for the users\n",
    "\n",
    "(2) Probability of each items to recommend for the users\n",
    "\n",
    "(3) Yes/No on each items to recomeend for the users\n",
    "\n",
    "(4) Tensor of next items to recommend for the users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIdqVTjowZiR"
   },
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4CcOUEoInjD"
   },
   "source": [
    "## (2) Pointwise Feed Forward Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DCtgcHpGIpd"
   },
   "outputs": [],
   "source": [
    "class PointWiseFeedForward(torch.nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "\n",
    "        ##################Your code here ##################\n",
    "        ## Note:\n",
    "        ## 1. stack 2 Convolution Layers\n",
    "        ## 2. after each convolution layer, proceed dropout\n",
    "        ## 3. add relu after the first dropout and before the second convolution\n",
    "        ## Hint: use torch.nn.Conv1d with kernel_size=1\n",
    "\n",
    "        self.conv1 =\n",
    "        self.dropout1 =\n",
    "        self.relu =\n",
    "        self.conv2 =\n",
    "        self.dropout2 =\n",
    "\n",
    "        ###################################################\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.dropout2(self.conv2(self.relu(self.dropout1(self.conv1(inputs.transpose(-1, -2)))))) #transpose so kernel can pass by time steps\n",
    "        outputs = outputs.transpose(-1, -2) # return it back\n",
    "        outputs += inputs #Residual Connection\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duMEg-olLjbJ"
   },
   "source": [
    "## Question 5: The shape of output for the two convolution layer becomes batch_size(128) x d(50) x max_len(200). What will be the shape of the final output of the forward function after transpose and residual connection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbiOGL3QlFMu"
   },
   "source": [
    "Shape of final outputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgUA815bNJ8w"
   },
   "source": [
    "# 3 Evaluate on Test/Validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-8-kuVbepAS"
   },
   "outputs": [],
   "source": [
    "#randomly sample 100 negative items and rank these items with the ground truth item. Based on the rankings we can evaluate Hit@10 and NDCG@10\n",
    "def evaluate(model, dataset, selT, args):\n",
    "    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)\n",
    "\n",
    "    NDCG = 0.0 #Normalized Discounted Cumulative Gain: evaluates the ranked quality of recommendations\n",
    "               #by considering both position and relevance of the ground truth item in the recommendation list\n",
    "    HR = 0.0 #Hit Rate@k: measures the fraction of times the ground truth next item is among the top-k recommendations\n",
    "    valid_user = 0.0\n",
    "\n",
    "    if selT==True:\n",
    "      VT=test\n",
    "    else:\n",
    "      VT=valid\n",
    "\n",
    "    users = range(1,usernum+1)\n",
    "    for u in users:\n",
    "        if len(train[u]) < 1 or len(VT[u]) < 1: continue\n",
    "\n",
    "        seq = np.zeros([args['maxlen']], dtype=np.int32)\n",
    "        idx = args['maxlen'] - 1\n",
    "        if selT == True:\n",
    "          seq[idx] = valid[u][0]\n",
    "          idx -= 1\n",
    "\n",
    "        for i in reversed(train[u]):\n",
    "            seq[idx] = i\n",
    "            idx -= 1\n",
    "            if idx == -1: break\n",
    "\n",
    "        rated = set(train[u])\n",
    "        rated.add(0)\n",
    "        item_idx = [VT[u][0]]\n",
    "\n",
    "        for _ in range(100):\n",
    "            t = np.random.randint(1, itemnum + 1)\n",
    "            while t in rated: t = np.random.randint(1, itemnum + 1)\n",
    "            item_idx.append(t)\n",
    "\n",
    "        predictions = -model.predict(*[np.array(l) for l in [[u], [seq], item_idx]])\n",
    "        predictions = predictions[0]\n",
    "\n",
    "        ##################Your code here ##################\n",
    "        ## Note:\n",
    "        ## 1. create a rank by sorting 'predictions'\n",
    "        ## Hint: use argsort()\n",
    "\n",
    "        rank =\n",
    "\n",
    "        ###################################################\n",
    "\n",
    "        valid_user += 1\n",
    "\n",
    "        if rank < 10:\n",
    "            NDCG += 1 / np.log2(rank + 2)\n",
    "            HR += 1\n",
    "        if valid_user % 100 == 0:\n",
    "            print('.', end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return NDCG / valid_user, HR / valid_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IK9z0wQIwzQ"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Now we will implement our SASRec model!\n",
    "\n",
    "*Note: evaluation will take quite a while without a GPU (~ 15 minutes)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f6QslzefPZ3"
   },
   "outputs": [],
   "source": [
    "# Please do not change the args\n",
    "args={\n",
    "    'datapath':'https://drive.google.com/uc?id=1sjeWz4pXkVGmy__Tr8zCtGV6rPGZfgLy',\n",
    "    'batch_size': 128,\n",
    "    'lr': 0.001,\n",
    "    'maxlen': 50,\n",
    "    'hidden_units': 50,\n",
    "    'num_blocks': 2,\n",
    "    'num_epochs': 101,\n",
    "    'num_heads': 1,\n",
    "    'dropout_rate': 0.5,\n",
    "    'l2_emb': 0.0,\n",
    "    'device': 'cpu',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 850895,
     "status": "ok",
     "timestamp": 1696495333846,
     "user": {
      "displayName": "Jane HAN",
      "userId": "01838841618913860328"
     },
     "user_tz": -540
    },
    "id": "XuPGKYuP7ot2",
    "outputId": "6e74595a-b656-436d-c1d7-d79a8df1632e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating........................................................................................................................epoch:20, time: 131.297342(s), valid (NDCG@10: 0.4714, HR@10: 0.7334), test (NDCG@10: 0.4487, HR@10: 0.7101)\n",
      "Evaluating........................................................................................................................epoch:40, time: 259.095778(s), valid (NDCG@10: 0.5333, HR@10: 0.7886), test (NDCG@10: 0.5060, HR@10: 0.7565)\n",
      "Evaluating........................................................................................................................epoch:60, time: 390.139411(s), valid (NDCG@10: 0.5524, HR@10: 0.8045), test (NDCG@10: 0.5275, HR@10: 0.7757)\n",
      "Evaluating........................................................................................................................epoch:80, time: 521.176454(s), valid (NDCG@10: 0.5629, HR@10: 0.8083), test (NDCG@10: 0.5349, HR@10: 0.7823)\n",
      "Evaluating........................................................................................................................epoch:100, time: 649.208588(s), valid (NDCG@10: 0.5672, HR@10: 0.8108), test (NDCG@10: 0.5403, HR@10: 0.7873)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # global dataset\n",
    "    dataset = data_partition(args['datapath'])\n",
    "\n",
    "    [user_train, user_valid, user_test, usernum, itemnum] = dataset\n",
    "    num_batch = len(user_train) // args['batch_size']\n",
    "\n",
    "    sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args['batch_size'], maxlen=args['maxlen'], n_workers=3)\n",
    "    model = SASRec(usernum, itemnum, args).to(args['device'])\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        try: torch.nn.init.xavier_normal_(param.data)\n",
    "        except: pass\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_start_idx = 1\n",
    "    bce_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    adam_optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], betas=(0.9, 0.98))\n",
    "\n",
    "    T = 0.0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(epoch_start_idx, args['num_epochs'] + 1):\n",
    "        for step in range(num_batch):\n",
    "        ##################Your code here ##################\n",
    "        ## Note: implement a training loop\n",
    "        ## 1. get u, seq, pos, neg from sampler.next_batch()\n",
    "        ##    (see how we defined the WarpSampler)\n",
    "        ## 2. convert these to np.array objects.\n",
    "        ## 3. predict pos_logits and neg_logits by running model() with u, seq, pos, neg\n",
    "\n",
    "            u, seq, pos, neg =\n",
    "            u, seq, pos, neg =\n",
    "            pos_logits, neg_logits =\n",
    "\n",
    "         ###################################################\n",
    "\n",
    "            pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args['device']), torch.zeros(neg_logits.shape, device=args['device'])\n",
    "            adam_optimizer.zero_grad()\n",
    "            indices = np.where(pos != 0)\n",
    "            loss = bce_criterion(pos_logits[indices], pos_labels[indices])\n",
    "            loss += bce_criterion(neg_logits[indices], neg_labels[indices])\n",
    "            for param in model.item_emb.parameters(): loss += args['l2_emb'] * torch.norm(param)\n",
    "            loss.backward()\n",
    "            adam_optimizer.step()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            t1 = time.time() - t0\n",
    "            T += t1\n",
    "            print('Evaluating', end='')\n",
    "            t_test = evaluate(model, dataset, True, args)\n",
    "            t_valid = evaluate(model, dataset, False, args)\n",
    "            print('epoch:%d, time: %f(s), valid (NDCG@10: %.4f, HR@10: %.4f), test (NDCG@10: %.4f, HR@10: %.4f)'\n",
    "                    % (epoch, T, t_valid[0], t_valid[1], t_test[0], t_test[1]))\n",
    "            t0 = time.time()\n",
    "            model.train()\n",
    "\n",
    "    sampler.close()\n",
    "    print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cms4486Bc3Ao"
   },
   "source": [
    "## Question 6: What are the test NDCG@10 and HR@10 for SASRec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aND1ZFdRAJuz"
   },
   "source": [
    "NDCG@10:\n",
    "\n",
    "HR@10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7JXsMTBgeOI"
   },
   "source": [
    "# Submission\n",
    "\n",
    "In order to get credit, you need to submit the `ipynb` file to LMS.\n",
    "\n",
    "To get this file, click `File` and `Download .ipynb`. Please make sure that your output of each cell is available in your `ipynb` file."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1VUP7iAhPsf0CNdyaTiK4gZLAETKEaIFl",
     "timestamp": 1695258082812
    },
    {
     "file_id": "1Aa0eKSmyYef1gORvlHv7EeQzSVRb30eL",
     "timestamp": 1623246611233
    },
    {
     "file_id": "1Jc5CAEGZIvY0vka3mBdf0tqn2TaJr2O1",
     "timestamp": 1610408674518
    },
    {
     "file_id": "1gc6u6hItUKY9uJt6GXHaneSYCMaGcxp1",
     "timestamp": 1610395347938
    },
    {
     "file_id": "1CqWY4pk7_VFxi8K8v4asr18ed0Hs8FVA",
     "timestamp": 1578441204356
    }
   ]
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
