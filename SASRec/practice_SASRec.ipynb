{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T06:07:29.538194Z",
     "start_time": "2023-11-11T06:07:29.057989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                2.1.0\r\n",
      "torch_geometric      2.4.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T08:57:58.696832Z",
     "start_time": "2023-11-11T08:57:58.689649Z"
    }
   },
   "outputs": [],
   "source": [
    "attention_layers = torch.nn.ModuleList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T08:59:02.908527Z",
     "start_time": "2023-11-11T08:59:02.903308Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please do not change the args\n",
    "args={\n",
    "    'datapath':'https://drive.google.com/uc?id=1sjeWz4pXkVGmy__Tr8zCtGV6rPGZfgLy',\n",
    "    'batch_size': 128,\n",
    "    'lr': 0.001,\n",
    "    'maxlen': 50,\n",
    "    'hidden_units': 50,\n",
    "    'num_blocks': 2,\n",
    "    'num_epochs': 101,\n",
    "    'num_heads': 1,\n",
    "    'dropout_rate': 0.5,\n",
    "    'l2_emb': 0.0,\n",
    "    'device': 'cpu',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T08:59:05.435774Z",
     "start_time": "2023-11-11T08:59:05.408101Z"
    }
   },
   "outputs": [],
   "source": [
    "new_attn_layer =  torch.nn.MultiheadAttention(args['hidden_units'],\n",
    "                                                            args['num_heads'],\n",
    "                                                            args['dropout_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T09:00:15.523044Z",
     "start_time": "2023-11-11T09:00:15.516894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-1): 2 x MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    attention_layers.append(new_attn_layer)\n",
    "\n",
    "attention_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T09:00:20.668866Z",
     "start_time": "2023-11-11T09:00:20.461155Z"
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [ModuleList] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mattention_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:372\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Module [ModuleList] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T08:39:33.177457Z",
     "start_time": "2023-11-11T08:39:33.174879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuXWJLEm2UWS"
   },
   "source": [
    "# **AAI0026 Practice : SASRec**\n",
    "## **Self-Attentive Sequential Recommendation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGKqVEbbMEzf"
   },
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T06:08:45.726677Z",
     "start_time": "2023-11-11T06:08:43.194420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0NiFL6OLpaJ"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T06:09:55.485501Z",
     "start_time": "2023-11-11T06:09:55.480568Z"
    },
    "id": "By2oyBw7Lrh5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Process, Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nwwq0nSdmsOL"
   },
   "source": [
    "# 1 Input Action Sequence\n",
    "- Load MovieLens Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T06:19:18.758760Z",
     "start_time": "2023-11-11T06:19:18.737931Z"
    },
    "id": "vCmgBy-2VgJs"
   },
   "outputs": [],
   "source": [
    "def data_partition(data_path):\n",
    "   # TODO: Implement this function that takes a text file,\n",
    "   # convert the text file to the dictionary,\n",
    "   # and then split the dictionary to the training, validation, and test data\n",
    "\n",
    "    usernum = 0\n",
    "    itemnum = 0\n",
    "    User = defaultdict(list) # create a dictionary with default value as empty list\n",
    "    user_train = {}\n",
    "    user_valid = {}\n",
    "    user_test = {}\n",
    "\n",
    "    os.system(f'wget {data_path} -O data.txt')\n",
    "    with open('data.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            u, i = map(int, line.rstrip().split(' '))\n",
    "            User[u].append(i)\n",
    "\n",
    "        usernum = max(User.keys())\n",
    "        itemnum = max(max(User.values()))\n",
    "\n",
    "    # The second-last and the last movies are for validation and test data respectively\n",
    "        for user in User:\n",
    "            nfeedback = len(User[user])\n",
    "            if nfeedback < 3:\n",
    "                user_train[user] = User[user]\n",
    "                user_valid[user] = []\n",
    "                user_test[user] = []\n",
    "            else:\n",
    "                user_train[user] = User[user][:-2]\n",
    "                user_valid[user] = []\n",
    "                user_valid[user].append(User[user][-2])\n",
    "                user_test[user] = []\n",
    "                user_test[user].append(User[user][-1])\n",
    "    return [user_train, user_valid, user_test, usernum, itemnum]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sCV3xJWCddX"
   },
   "source": [
    "## Question 1: What is the number of Users and Items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T06:19:31.115354Z",
     "start_time": "2023-11-11T06:19:27.400374Z"
    },
    "id": "1Np-FxA6tG1J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-11-11 15:19:27--  https://drive.google.com/uc?id=1sjeWz4pXkVGmy__Tr8zCtGV6rPGZfgLy\n",
      "Resolving drive.google.com (drive.google.com)... 142.250.76.142\n",
      "Connecting to drive.google.com (drive.google.com)|142.250.76.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-08-1g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ulftp91r0vijbn701jl7053rq618gjon/1699683525000/01838841618913860328/*/1sjeWz4pXkVGmy__Tr8zCtGV6rPGZfgLy?uuid=ca86503a-6f61-4d0d-917d-5f649c687140 [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2023-11-11 15:19:28--  https://doc-08-1g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ulftp91r0vijbn701jl7053rq618gjon/1699683525000/01838841618913860328/*/1sjeWz4pXkVGmy__Tr8zCtGV6rPGZfgLy?uuid=ca86503a-6f61-4d0d-917d-5f649c687140\n",
      "Resolving doc-08-1g-docs.googleusercontent.com (doc-08-1g-docs.googleusercontent.com)... 142.250.206.225\n",
      "Connecting to doc-08-1g-docs.googleusercontent.com (doc-08-1g-docs.googleusercontent.com)|142.250.206.225|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9053831 (8.6M) [text/plain]\n",
      "Saving to: `data.txt'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0%  780K 11s\n",
      "    50K .......... .......... .......... .......... ..........  1%  986K 10s\n",
      "   100K .......... .......... .......... .......... ..........  1% 3.04M 8s\n",
      "   150K .......... .......... .......... .......... ..........  2% 2.98M 6s\n",
      "   200K .......... .......... .......... .......... ..........  2% 3.12M 6s\n",
      "   250K .......... .......... .......... .......... ..........  3% 4.35M 5s\n",
      "   300K .......... .......... .......... .......... ..........  3% 4.69M 4s\n",
      "   350K .......... .......... .......... .......... ..........  4% 5.18M 4s\n",
      "   400K .......... .......... .......... .......... ..........  5% 10.7M 4s\n",
      "   450K .......... .......... .......... .......... ..........  5% 9.22M 3s\n",
      "   500K .......... .......... .......... .......... ..........  6% 9.47M 3s\n",
      "   550K .......... .......... .......... .......... ..........  6% 10.3M 3s\n",
      "   600K .......... .......... .......... .......... ..........  7% 8.09M 3s\n",
      "   650K .......... .......... .......... .......... ..........  7% 18.2M 3s\n",
      "   700K .......... .......... .......... .......... ..........  8% 9.74M 2s\n",
      "   750K .......... .......... .......... .......... ..........  9% 16.1M 2s\n",
      "   800K .......... .......... .......... .......... ..........  9% 13.2M 2s\n",
      "   850K .......... .......... .......... .......... .......... 10% 10.6M 2s\n",
      "   900K .......... .......... .......... .......... .......... 10% 18.7M 2s\n",
      "   950K .......... .......... .......... .......... .......... 11% 17.9M 2s\n",
      "  1000K .......... .......... .......... .......... .......... 11% 19.0M 2s\n",
      "  1050K .......... .......... .......... .......... .......... 12% 15.9M 2s\n",
      "  1100K .......... .......... .......... .......... .......... 13% 17.4M 2s\n",
      "  1150K .......... .......... .......... .......... .......... 13% 67.8M 2s\n",
      "  1200K .......... .......... .......... .......... .......... 14% 20.8M 2s\n",
      "  1250K .......... .......... .......... .......... .......... 14% 18.9M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 15% 20.3M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 15% 51.1M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 16% 30.7M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 16% 18.7M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 17% 25.4M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 18% 42.2M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 18% 18.8M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 19% 75.6M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 19% 25.3M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 20% 18.6M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 20%  313M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 21% 22.2M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 22% 27.3M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 22% 25.7M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 23%  341M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 23% 42.9M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 24% 24.0M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 24% 26.7M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 25% 48.9M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 26% 32.7M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 26% 54.4M 1s\n",
      "  2350K .......... .......... .......... .......... .......... 27% 33.0M 1s\n",
      "  2400K .......... .......... .......... .......... .......... 27% 27.3M 1s\n",
      "  2450K .......... .......... .......... .......... .......... 28%  332M 1s\n",
      "  2500K .......... .......... .......... .......... .......... 28% 56.6M 1s\n",
      "  2550K .......... .......... .......... .......... .......... 29% 32.1M 1s\n",
      "  2600K .......... .......... .......... .......... .......... 29% 49.9M 1s\n",
      "  2650K .......... .......... .......... .......... .......... 30% 38.6M 1s\n",
      "  2700K .......... .......... .......... .......... .......... 31% 23.8M 1s\n",
      "  2750K .......... .......... .......... .......... .......... 31% 35.3M 1s\n",
      "  2800K .......... .......... .......... .......... .......... 32%  448M 1s\n",
      "  2850K .......... .......... .......... .......... .......... 32% 50.8M 1s\n",
      "  2900K .......... .......... .......... .......... .......... 33%  503M 1s\n",
      "  2950K .......... .......... .......... .......... .......... 33% 26.4M 1s\n",
      "  3000K .......... .......... .......... .......... .......... 34% 64.0M 1s\n",
      "  3050K .......... .......... .......... .......... .......... 35% 41.0M 1s\n",
      "  3100K .......... .......... .......... .......... .......... 35% 74.0M 1s\n",
      "  3150K .......... .......... .......... .......... .......... 36% 45.7M 1s\n",
      "  3200K .......... .......... .......... .......... .......... 36% 80.3M 1s\n",
      "  3250K .......... .......... .......... .......... .......... 37%  182M 1s\n",
      "  3300K .......... .......... .......... .......... .......... 37% 94.8M 1s\n",
      "  3350K .......... .......... .......... .......... .......... 38% 24.8M 1s\n",
      "  3400K .......... .......... .......... .......... .......... 39%  461M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 39% 30.7M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 40% 51.1M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 40% 68.6M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 41% 58.0M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 41% 52.2M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 42% 71.7M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 42% 39.4M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 43% 36.0M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 44%  588M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 44% 22.8M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 45% 56.9M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 45% 60.1M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 46%  311K 1s\n",
      "  4100K .......... .......... .......... .......... .......... 46% 53.0M 1s\n",
      "  4150K .......... .......... .......... .......... .......... 47%  199M 1s\n",
      "  4200K .......... .......... .......... .......... .......... 48% 29.3M 1s\n",
      "  4250K .......... .......... .......... .......... .......... 48% 55.8M 1s\n",
      "  4300K .......... .......... .......... .......... .......... 49% 76.5M 1s\n",
      "  4350K .......... .......... .......... .......... .......... 49% 63.4M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 50% 22.6M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 50%  217M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 51% 49.4M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 52%  175M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 52% 48.6M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 53% 44.9M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 53% 1.11M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 54%  158M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 54% 73.3M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 55%  193M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 55%  157M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 56%  191M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 57% 32.2M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 57%  187M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 58%  158M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 58%  189M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 59% 25.4M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 59%  164M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 60% 34.3M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 61% 25.7M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 61% 95.9M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 62%  109M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 62% 18.1M 0s\n",
      "  5550K .......... .......... .......... .......... .......... 63% 42.9M 0s\n",
      "  5600K .......... .......... .......... .......... .......... 63% 99.2M 0s\n",
      "  5650K .......... .......... .......... .......... .......... 64%  111M 0s\n",
      "  5700K .......... .......... .......... .......... .......... 65%  228M 0s\n",
      "  5750K .......... .......... .......... .......... .......... 65%  136M 0s\n",
      "  5800K .......... .......... .......... .......... .......... 66%  146M 0s\n",
      "  5850K .......... .......... .......... .......... .......... 66% 58.4M 0s\n",
      "  5900K .......... .......... .......... .......... .......... 67% 28.4M 0s\n",
      "  5950K .......... .......... .......... .......... .......... 67%  241M 0s\n",
      "  6000K .......... .......... .......... .......... .......... 68%  198M 0s\n",
      "  6050K .......... .......... .......... .......... .......... 68% 50.3M 0s\n",
      "  6100K .......... .......... .......... .......... .......... 69%  244M 0s\n",
      "  6150K .......... .......... .......... .......... .......... 70% 94.4M 0s\n",
      "  6200K .......... .......... .......... .......... .......... 70% 32.3M 0s\n",
      "  6250K .......... .......... .......... .......... .......... 71%  119M 0s\n",
      "  6300K .......... .......... .......... .......... .......... 71%  110M 0s\n",
      "  6350K .......... .......... .......... .......... .......... 72%  111M 0s\n",
      "  6400K .......... .......... .......... .......... .......... 72% 1.17M 0s\n",
      "  6450K .......... .......... .......... .......... .......... 73% 36.3M 0s\n",
      "  6500K .......... .......... .......... .......... .......... 74% 99.9M 0s\n",
      "  6550K .......... .......... .......... .......... .......... 74%  117M 0s\n",
      "  6600K .......... .......... .......... .......... .......... 75% 86.6M 0s\n",
      "  6650K .......... .......... .......... .......... .......... 75% 97.3M 0s\n",
      "  6700K .......... .......... .......... .......... .......... 76% 90.6M 0s\n",
      "  6750K .......... .......... .......... .......... .......... 76% 60.2M 0s\n",
      "  6800K .......... .......... .......... .......... .......... 77% 93.9M 0s\n",
      "  6850K .......... .......... .......... .......... .......... 78%  105M 0s\n",
      "  6900K .......... .......... .......... .......... .......... 78%  105M 0s\n",
      "  6950K .......... .......... .......... .......... .......... 79% 94.1M 0s\n",
      "  7000K .......... .......... .......... .......... .......... 79% 90.6M 0s\n",
      "  7050K .......... .......... .......... .......... .......... 80%  117M 0s\n",
      "  7100K .......... .......... .......... .......... .......... 80%  224M 0s\n",
      "  7150K .......... .......... .......... .......... .......... 81%  187M 0s\n",
      "  7200K .......... .......... .......... .......... .......... 81%  211M 0s\n",
      "  7250K .......... .......... .......... .......... .......... 82% 27.7M 0s\n",
      "  7300K .......... .......... .......... .......... .......... 83%  208M 0s\n",
      "  7350K .......... .......... .......... .......... .......... 83%  223M 0s\n",
      "  7400K .......... .......... .......... .......... .......... 84%  193M 0s\n",
      "  7450K .......... .......... .......... .......... .......... 84% 70.7M 0s\n",
      "  7500K .......... .......... .......... .......... .......... 85%  191M 0s\n",
      "  7550K .......... .......... .......... .......... .......... 85%  174M 0s\n",
      "  7600K .......... .......... .......... .......... .......... 86% 26.4M 0s\n",
      "  7650K .......... .......... .......... .......... .......... 87%  138M 0s\n",
      "  7700K .......... .......... .......... .......... .......... 87% 1.44M 0s\n",
      "  7750K .......... .......... .......... .......... .......... 88%  143M 0s\n",
      "  7800K .......... .......... .......... .......... .......... 88%  163M 0s\n",
      "  7850K .......... .......... .......... .......... .......... 89%  197M 0s\n",
      "  7900K .......... .......... .......... .......... .......... 89%  195M 0s\n",
      "  7950K .......... .......... .......... .......... .......... 90%  157M 0s\n",
      "  8000K .......... .......... .......... .......... .......... 91%  187M 0s\n",
      "  8050K .......... .......... .......... .......... .......... 91%  178M 0s\n",
      "  8100K .......... .......... .......... .......... .......... 92%  191M 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8150K .......... .......... .......... .......... .......... 92%  374K 0s\n",
      "  8200K .......... .......... .......... .......... .......... 93% 26.2M 0s\n",
      "  8250K .......... .......... .......... .......... .......... 93% 26.3M 0s\n",
      "  8300K .......... .......... .......... .......... .......... 94% 8.23M 0s\n",
      "  8350K .......... .......... .......... .......... .......... 95% 18.5M 0s\n",
      "  8400K .......... .......... .......... .......... .......... 95% 52.6M 0s\n",
      "  8450K .......... .......... .......... .......... .......... 96% 38.0M 0s\n",
      "  8500K .......... .......... .......... .......... .......... 96% 62.4M 0s\n",
      "  8550K .......... .......... .......... .......... .......... 97% 33.5M 0s\n",
      "  8600K .......... .......... .......... .......... .......... 97% 29.1M 0s\n",
      "  8650K .......... .......... .......... .......... .......... 98% 27.6M 0s\n",
      "  8700K .......... .......... .......... .......... .......... 98%  190M 0s\n",
      "  8750K .......... .......... .......... .......... .......... 99% 63.0M 0s\n",
      "  8800K .......... .......... .......... .......... .         100% 28.5M=0.8s\n",
      "\n",
      "2023-11-11 15:19:30 (10.6 MB/s) - `data.txt' saved [9053831/9053831]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users: 6040, number of items: 3412\n"
     ]
    }
   ],
   "source": [
    "data_path = 'https://drive.google.com/uc?id=1sjeWz4pXkVGmy__Tr8zCtGV6rPGZfgLy'\n",
    "user_train, user_valid, user_test, usernum, itemnum = data_partition(data_path)\n",
    "\n",
    "print(f'number of users: {usernum}, number of items: {itemnum}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T06:20:29.081732Z",
     "start_time": "2023-11-11T06:20:29.074802Z"
    }
   },
   "source": [
    "전체 유저수 = 6040개, 아이템개수는 3412개 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqOcxa0vtFEA"
   },
   "source": [
    "## Question 2: What is the ID of the movie that User 26 has watched most recently?\n",
    "### (use dictionary user_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T14:40:38.477010Z",
     "start_time": "2023-10-31T14:40:38.471855Z"
    },
    "id": "ATGeg8jWt8VP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last movie that User 26 has watched: [901]\n"
     ]
    }
   ],
   "source": [
    "print(f'The last movie that User 26 has watched: {user_test[26]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T06:37:08.198611Z",
     "start_time": "2023-11-11T06:37:08.193941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4,\n",
       " 8,\n",
       " 10,\n",
       " 12,\n",
       " 16,\n",
       " 17,\n",
       " 20,\n",
       " 24,\n",
       " 25,\n",
       " 44,\n",
       " 46,\n",
       " 86,\n",
       " 146,\n",
       " 149,\n",
       " 151,\n",
       " 154,\n",
       " 240,\n",
       " 273,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 284,\n",
       " 285,\n",
       " 288,\n",
       " 289,\n",
       " 293,\n",
       " 305,\n",
       " 307,\n",
       " 313,\n",
       " 315,\n",
       " 320,\n",
       " 322,\n",
       " 324,\n",
       " 340,\n",
       " 342,\n",
       " 343,\n",
       " 358,\n",
       " 371,\n",
       " 375,\n",
       " 378,\n",
       " 387,\n",
       " 390,\n",
       " 395,\n",
       " 400,\n",
       " 420,\n",
       " 453,\n",
       " 459,\n",
       " 461,\n",
       " 465,\n",
       " 474,\n",
       " 475,\n",
       " 480,\n",
       " 485,\n",
       " 491,\n",
       " 496,\n",
       " 499,\n",
       " 501,\n",
       " 507,\n",
       " 510,\n",
       " 511,\n",
       " 514,\n",
       " 522,\n",
       " 523,\n",
       " 528,\n",
       " 529,\n",
       " 532,\n",
       " 545,\n",
       " 549,\n",
       " 559,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while t in set(user_train[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:27:58.848251Z",
     "start_time": "2023-11-11T07:27:58.827592Z"
    },
    "code_folding": [],
    "id": "F_vhRYIXYbmy"
   },
   "outputs": [],
   "source": [
    "#Negative Sampling: select items not in the set of positive items\n",
    "def random_neq(l, r, s):\n",
    "    ##TODO: Implement this function that takes two indices 'l', 'r', and set 's',\n",
    "    ##and return a random integer number t in the range [l,r]\n",
    "    ##but not in set 's' so it can generate a new random number\n",
    "\n",
    "    t = np.random.randint(l, r)\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. to ensure that t is not in set 's', use while loop\n",
    "    ## 2. repeat generating random numbers until t is not in set 's'\n",
    "    ## 3. (~3 lines of code)\n",
    "    while t in s:\n",
    "        t = np.random.randint(l, r)\n",
    "    return t\n",
    "\n",
    "    #########################################\n",
    "\n",
    "#generate seg, pos, neg numpy ndarrays.\n",
    "#they are the fixed-length sequences ('maxlen')\n",
    "\n",
    "def sample_function(user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED):\n",
    "    def sample():\n",
    "    ##TODO: Implement this function that takes arguments,\n",
    "    ##and return numpy ndarrays user, seq, pos and neg\n",
    "\n",
    "        user = np.random.randint(1, usernum + 1)\n",
    "        while len(user_train[user]) <= 1: user = np.random.randint(1, usernum + 1)\n",
    "\n",
    "        seq = np.zeros([maxlen], dtype=np.int32) #zeros for padding\n",
    "        pos = np.zeros([maxlen], dtype=np.int32)\n",
    "        neg = np.zeros([maxlen], dtype=np.int32)\n",
    "        nxt = user_train[user][-1]\n",
    "        idx = maxlen - 1\n",
    "\n",
    "        ts = set(user_train[user]) #positive item set\n",
    "        for i in reversed(user_train[user][:-1]): #reverse for padding from the left\n",
    "\n",
    "            seq[idx] = i\n",
    "            pos[idx] = nxt\n",
    "\n",
    "            if nxt != 0:\n",
    "            ############# Your code here ############\n",
    "            ##Note:\n",
    "            ##1. generate neg Numpy ndarray by using random_neg(l,r,s)\n",
    "            ##2. fill in neg[idx] by using the for loop we are in\n",
    "            ##3. remember item ID starts from 1\n",
    "            ##4. (~1 line of code)\n",
    "                neg_item = random_neq(1, itemnum + 1, ts)\n",
    "                neg[idx] = neg_item\n",
    "\n",
    "\n",
    "            #########################################\n",
    "            nxt = i\n",
    "            idx -= 1\n",
    "            if idx == -1: break\n",
    "\n",
    "        return (user, seq, pos, neg)\n",
    "\n",
    "    # Make sure random numbers generated are always the same everytime\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    while True:\n",
    "        one_batch = []\n",
    "        for i in range(batch_size):\n",
    "            one_batch.append(sample())\n",
    "        result_queue.put(zip(*one_batch))\n",
    "\n",
    "class WarpSampler(object):\n",
    "    def __init__(self, User, usernum, itemnum, batch_size=64, maxlen=10, n_workers=1):\n",
    "    ##TODO: Implement this function that takes arguments,\n",
    "    ##and process sampling in parallel using multiple worker processes\n",
    "\n",
    "        self.result_queue = Queue(maxsize=n_workers*10)\n",
    "        self.processors = []\n",
    "        \n",
    "        sample_function(User,usernum,itemnum,batch_size,maxlen,self.result_queue,np.random.randint(2e9))\n",
    "        for i in range(n_workers):\n",
    "\n",
    "            ############# Your code here ###################################\n",
    "            ##Note:\n",
    "            ##1. append \"Process(target, args)\" object in self.processors\n",
    "            ##2. set target to \"sample_function\"\n",
    "            ##3. create a tuple of all arguemnts of sample_function,\n",
    "            ##.  result_queue='self.result_queue', and SEED='np.random.randint(2e9)'.\n",
    "            ##4. set args to this tuple\n",
    "#             def sample_function(user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED):\n",
    "#             sampler = WarpSampler(user_train, usernum, itemnum, batch_size=64, maxlen=10, n_workers=3)\n",
    "                d\n",
    "#             self.processors.append(\n",
    "#                 Process(target=sample_function, args=(User,usernum,itemnum,batch_size,maxlen,self.result_queue,np.random.randint(2e9)))\n",
    "#             )\n",
    "            ################################################################\n",
    "\n",
    "#             self.processors[-1].daemon = True #doesn't prevent the program from exiting\n",
    "#             self.processors[-1].start() #To begine the Multiprocess\n",
    "\n",
    "    #get result_queue when it becomes available, without waiting for all processes to finish\n",
    "    def next_batch(self):\n",
    "        return self.result_queue.get()\n",
    "\n",
    "#     def close(self):\n",
    "#         for p in self.processors:\n",
    "#             p.terminate()\n",
    "#             p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKhcVeAhCwoY"
   },
   "source": [
    "## Question 3: Does our sampler work well for User 26?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:28:48.806023Z",
     "start_time": "2023-11-11T07:28:03.076452Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1696494444898,
     "user": {
      "displayName": "Jane HAN",
      "userId": "01838841618913860328"
     },
     "user_tz": -540
    },
    "id": "jPWl0hDO341V",
    "outputId": "60b00d9f-9316-4fa3-98c1-8b17f61d7a58"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [84]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sampler \u001b[38;5;241m=\u001b[39m \u001b[43mWarpSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musernum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitemnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# Check one batch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     u, seq, pos, neg \u001b[38;5;241m=\u001b[39m sampler\u001b[38;5;241m.\u001b[39mnext_batch()\n",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36mWarpSampler.__init__\u001b[0;34m(self, User, usernum, itemnum, batch_size, maxlen, n_workers)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult_queue \u001b[38;5;241m=\u001b[39m Queue(maxsize\u001b[38;5;241m=\u001b[39mn_workers\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 78\u001b[0m         \u001b[43msample_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUser\u001b[49m\u001b[43m,\u001b[49m\u001b[43musernum\u001b[49m\u001b[43m,\u001b[49m\u001b[43mitemnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2e9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_workers):\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m             \u001b[38;5;66;03m############# Your code here ###################################\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#             def sample_function(user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED):\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m#             sampler = WarpSampler(user_train, usernum, itemnum, batch_size=64, maxlen=10, n_workers=3)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m                 d\n",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36msample_function\u001b[0;34m(user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m     67\u001b[0m     one_batch\u001b[38;5;241m.\u001b[39mappend(sample())\n\u001b[0;32m---> 68\u001b[0m \u001b[43mresult_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/multiprocessing/queues.py:89\u001b[0m, in \u001b[0;36mQueue.put\u001b[0;34m(self, obj, block, timeout)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueue \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is closed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Full\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_notempty:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampler = WarpSampler(user_train, usernum, itemnum, batch_size=64, maxlen=10, n_workers=3)\n",
    "\n",
    "for _ in range(1):  # Check one batch\n",
    "    u, seq, pos, neg = sampler.next_batch()\n",
    "\n",
    "    ############# Your code here ###################################\n",
    "    ##note:\n",
    "    ##1.use any() function\n",
    "    ##2.check if neg[26] items are in pos[26]\n",
    "\n",
    "    same_item =any(neg[26] == item for item in pos[26])\n",
    "\n",
    "    #################################################################\n",
    "    if same_item==False:\n",
    "        print(\"Works Well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2Uhrl6Mqsp2"
   },
   "source": [
    "# 2 SASRec Model\n",
    "\n",
    "Now we will implement our SASRec model!\n",
    "\n",
    "Please see the following:\n",
    "\n",
    "1.   Embedding Layer (Item Embedding, Position Embedding):\n",
    "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=16w1sfleDGnnKEQvL-wwPO4REwt3te1TA\">\n",
    "</p>\n",
    "\n",
    "  We will obtain an `item embedding` and a learnable `position embedding` by using this layer. Every item ID in `seq` turns into its item embedding, which gives Tensor(M) of `item embeddings` in shape batch_size(128) x max_len(200) x d(50). Since the self-attention model doesn't include any recurrent or convolutional module, it's impossible to be aware of the positions of previous items. Hence we inject a `position embedding` that represents the position of value in `seq`, i.e., max_len (200) positions ranging 0~199. The positions are encoded to Tensor(P) with the same shape as M. Final `input embedding(E=M+P)` serves as Key and Value, and the layer normalization of input embedding serves as Query.\n",
    "\n",
    "2.   Self-Attention Layer:\n",
    "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=1UuTRCQuCQjf3p49wxPO4z3_95xGE0SqR\">\n",
    "</p>\n",
    "\n",
    "     - Feed (Q,K,V) into Attention layer and compute `attention scores(S)`\n",
    "     - when proceeding dot production of Query and Key (Q*K), need `Attention Masking` to avoid cheeting\n",
    "     - Residual Connection\n",
    "     - Layer Normalization\n",
    "\n",
    "3.   Pointwise Feed Forward Network:\n",
    "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=1bij9ohJtxj4UhpB9OcjRnK8rw9bQLqVz\">\n",
    "</p>\n",
    "\n",
    "  Feed Forward Network consists of \"two 1d Convolution Layers\" which functions as linear transformation. Before passing 1d-Convolution Layer, input data need to be transposed to allow the kernel to move across time-steps. These Convolution Layers are linked with `ReLU activation` and their outputs will be added with the input data for Residual Connection. After Forward Layer the masking (timeline mask), which was used in item embedding, is applied to preserve their initial padding positions.    \n",
    "\n",
    "4.   Prediction Layer:\n",
    "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=14ACxSsuyxeKdtA8QG2HN_XGzV0-p_xSc\">\n",
    "</p>\n",
    "\n",
    "  Finally, Prediction Layer generates `pos_logits` and `neg_logits`, i.e., the scores of movies in positive sets and negative sets respectively for users.\n",
    "  We compute the \"element-wise product\" between pos embedding and attention scores (`log_feats`) obtained in Attention Layer to generate `pos_logits`. We also compute the element-wise product between neg_embedding and the attention score to generate `neg_logits`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQqKUwqU5Qf8"
   },
   "source": [
    "## (1) Build Layers for SASRec\n",
    "- Embedding Layer\n",
    "- Self-Attention Layer\n",
    "- Forward Layer\n",
    "- Prediction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEsLOTQkpsUC"
   },
   "outputs": [],
   "source": [
    "class SASRec(torch.nn.Module):\n",
    "    def __init__(self, user_num, item_num, args):\n",
    "        super(SASRec, self).__init__()\n",
    "\n",
    "        ##TODO: stack embedding layer, attention layer and forward layer,\n",
    "        ##define the function that computes Attention Scores(log_feat)\n",
    "        ##and get pos_logits, neg_logits from FFN and Prediction\n",
    "\n",
    "        self.user_num = user_num\n",
    "        self.item_num = item_num\n",
    "        self.dev = args['device']\n",
    "\n",
    "        self.item_emb = torch.nn.Embedding(self.item_num+1, args['hidden_units'], padding_idx=0)\n",
    "\n",
    "        ##################Your code here ##################\n",
    "        ## Note:\n",
    "        ## 1. Define position embedding and dropout\n",
    "        ## 2. See how item embedding is created above\n",
    "        ## 3. use arg['maxlen'], args['hidden_units'], args['dropout_rate']\n",
    "        ## 4. position embedding doesn't need padding idx\n",
    "\n",
    "        self.pos_emb =\n",
    "        self.emb_dropout =\n",
    "\n",
    "        ###################################################\n",
    "\n",
    "        self.attention_layernorms = torch.nn.ModuleList() # to be Query for self-attention\n",
    "        self.attention_layers = torch.nn.ModuleList() #multi-head for self-attention\n",
    "        self.forward_layernorms = torch.nn.ModuleList()\n",
    "        self.forward_layers = torch.nn.ModuleList()\n",
    "        self.last_layernorm = torch.nn.LayerNorm(args['hidden_units'], eps=1e-8)\n",
    "\n",
    "        for _ in range(args['num_blocks']): #stacks 2 blocks\n",
    "\n",
    "            new_attn_layernorm = torch.nn.LayerNorm(args['hidden_units'], eps=1e-8)\n",
    "            self.attention_layernorms.append(new_attn_layernorm)\n",
    "\n",
    "            new_attn_layer =  torch.nn.MultiheadAttention(args['hidden_units'],\n",
    "                                                            args['num_heads'],\n",
    "                                                            args['dropout_rate'])\n",
    "            self.attention_layers.append(new_attn_layer)\n",
    "\n",
    "            new_fwd_layernorm = torch.nn.LayerNorm(args['hidden_units'], eps=1e-8)\n",
    "            self.forward_layernorms.append(new_fwd_layernorm)\n",
    "\n",
    "            new_fwd_layer = PointWiseFeedForward(args['hidden_units'], args['dropout_rate'])\n",
    "            self.forward_layers.append(new_fwd_layer)\n",
    "\n",
    "            self.pos_sigmoid = torch.nn.Sigmoid()\n",
    "            self.neg_sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def log2feats(self, log_seqs):\n",
    "        ## TODO: Implement this function that takes log_seqs,\n",
    "        ## and make input tensor(item embedding + position embedding) for self-attention layer\n",
    "        ## then return attention scores(log_feats) tensor from self-attention layer\n",
    "\n",
    "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.dev))\n",
    "        seqs *= self.item_emb.embedding_dim ** 0.5 #scaling to stabilize the training process\n",
    "\n",
    "        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1])\n",
    "\n",
    "        ##################Your code here ##################\n",
    "        ## Note:\n",
    "        ## 1. Sum position embedding to input embedding(seqs)\n",
    "        ## 2. Proceed the dropout after\n",
    "        ## 3. (~2 lines of code)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ###################################################\n",
    "\n",
    "        timeline_mask = torch.BoolTensor(log_seqs == 0).to(self.dev)\n",
    "        seqs *= ~timeline_mask.unsqueeze(-1) # broadcast in last dim\n",
    "\n",
    "        tl = seqs.shape[1] # time dim len for enforce causality\n",
    "        attention_mask = ~torch.tril(torch.ones((tl, tl), dtype=torch.bool, device=self.dev))\n",
    "        #to mask upper triangular part\n",
    "\n",
    "        for i in range(len(self.attention_layers)):\n",
    "            seqs = torch.transpose(seqs, 0, 1)\n",
    "\n",
    "           ##################Your code here ##################\n",
    "           ## Note:\n",
    "           ## 1. get Q by attention_layernorms[]()\n",
    "           ## 2. get multihead attention outputs (mha_outputs): sum of the weighted V\n",
    "           ##    by attention_layers[]() with using attention_mask\n",
    "           ## 3. key, value = seqs\n",
    "\n",
    "            Q =\n",
    "            mha_outputs, _ =\n",
    "\n",
    "            ###################################################\n",
    "            seqs = Q + mha_outputs\n",
    "            seqs = torch.transpose(seqs, 0, 1)\n",
    "\n",
    "            seqs = self.forward_layernorms[i](seqs)\n",
    "            seqs = self.forward_layers[i](seqs)\n",
    "            seqs *=  ~timeline_mask.unsqueeze(-1)\n",
    "\n",
    "        log_feats = self.last_layernorm(seqs)\n",
    "\n",
    "        return log_feats #Attention Scores\n",
    "\n",
    "    def forward(self, user_ids, log_seqs, pos_seqs, neg_seqs):\n",
    "        log_feats = self.log2feats(log_seqs)\n",
    "\n",
    "        pos_embs = self.item_emb(torch.LongTensor(pos_seqs).to(self.dev))\n",
    "        neg_embs = self.item_emb(torch.LongTensor(neg_seqs).to(self.dev))\n",
    "\n",
    "        ##################Your code here ##################\n",
    "        ## Note: get pos_logits and neg_logits\n",
    "        ## 1.compute Attention Scores * Value(pos_embs/neg_embs)\n",
    "        ## Hint: use sum(dim=-1)\n",
    "        ## (~2 lines of code)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ###################################################\n",
    "\n",
    "        pos_pred = self.pos_sigmoid(pos_logits)\n",
    "        neg_pred = self.neg_sigmoid(neg_logits)\n",
    "\n",
    "        return pos_logits, neg_logits\n",
    "\n",
    "    def predict(self, user_ids, log_seqs, item_indices):\n",
    "        log_feats = self.log2feats(log_seqs)\n",
    "\n",
    "        final_feat = log_feats[:, -1, :] # the final Attention Scores for the prediction\n",
    "\n",
    "        item_embs = self.item_emb(torch.LongTensor(item_indices).to(self.dev))\n",
    "\n",
    "        logits = item_embs.matmul(final_feat.unsqueeze(-1)).squeeze(-1) #dot product between item's embedding and final feature\n",
    "                                                                        #squeeze to make dimensions match\n",
    "        preds = self.pos_sigmoid(logits)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUu28G0ihX2J"
   },
   "source": [
    "## Question 4: What does the output (preds) of SASRec mean?\n",
    "\n",
    "(1) list of next items to recommend for the users\n",
    "\n",
    "(2) Probability of each items to recommend for the users\n",
    "\n",
    "(3) Yes/No on each items to recomeend for the users\n",
    "\n",
    "(4) Tensor of next items to recommend for the users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIdqVTjowZiR"
   },
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4CcOUEoInjD"
   },
   "source": [
    "## (2) Pointwise Feed Forward Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DCtgcHpGIpd"
   },
   "outputs": [],
   "source": [
    "class PointWiseFeedForward(torch.nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "\n",
    "        ##################Your code here ##################\n",
    "        ## Note:\n",
    "        ## 1. stack 2 Convolution Layers\n",
    "        ## 2. after each convolution layer, proceed dropout\n",
    "        ## 3. add relu after the first dropout and before the second convolution\n",
    "        ## Hint: use torch.nn.Conv1d with kernel_size=1\n",
    "\n",
    "        self.conv1 =\n",
    "        self.dropout1 =\n",
    "        self.relu =\n",
    "        self.conv2 =\n",
    "        self.dropout2 =\n",
    "\n",
    "        ###################################################\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.dropout2(self.conv2(self.relu(self.dropout1(self.conv1(inputs.transpose(-1, -2)))))) #transpose so kernel can pass by time steps\n",
    "        outputs = outputs.transpose(-1, -2) # return it back\n",
    "        outputs += inputs #Residual Connection\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duMEg-olLjbJ"
   },
   "source": [
    "## Question 5: The shape of output for the two convolution layer becomes batch_size(128) x d(50) x max_len(200). What will be the shape of the final output of the forward function after transpose and residual connection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbiOGL3QlFMu"
   },
   "source": [
    "Shape of final outputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgUA815bNJ8w"
   },
   "source": [
    "# 3 Evaluate on Test/Validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-8-kuVbepAS"
   },
   "outputs": [],
   "source": [
    "#randomly sample 100 negative items and rank these items with the ground truth item. Based on the rankings we can evaluate Hit@10 and NDCG@10\n",
    "def evaluate(model, dataset, selT, args):\n",
    "    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)\n",
    "\n",
    "    NDCG = 0.0 #Normalized Discounted Cumulative Gain: evaluates the ranked quality of recommendations\n",
    "               #by considering both position and relevance of the ground truth item in the recommendation list\n",
    "    HR = 0.0 #Hit Rate@k: measures the fraction of times the ground truth next item is among the top-k recommendations\n",
    "    valid_user = 0.0\n",
    "\n",
    "    if selT==True:\n",
    "      VT=test\n",
    "    else:\n",
    "      VT=valid\n",
    "\n",
    "    users = range(1,usernum+1)\n",
    "    for u in users:\n",
    "        if len(train[u]) < 1 or len(VT[u]) < 1: continue\n",
    "\n",
    "        seq = np.zeros([args['maxlen']], dtype=np.int32)\n",
    "        idx = args['maxlen'] - 1\n",
    "        if selT == True:\n",
    "          seq[idx] = valid[u][0]\n",
    "          idx -= 1\n",
    "\n",
    "        for i in reversed(train[u]):\n",
    "            seq[idx] = i\n",
    "            idx -= 1\n",
    "            if idx == -1: break\n",
    "\n",
    "        rated = set(train[u])\n",
    "        rated.add(0)\n",
    "        item_idx = [VT[u][0]]\n",
    "\n",
    "        for _ in range(100):\n",
    "            t = np.random.randint(1, itemnum + 1)\n",
    "            while t in rated: t = np.random.randint(1, itemnum + 1)\n",
    "            item_idx.append(t)\n",
    "\n",
    "        predictions = -model.predict(*[np.array(l) for l in [[u], [seq], item_idx]])\n",
    "        predictions = predictions[0]\n",
    "\n",
    "        ##################Your code here ##################\n",
    "        ## Note:\n",
    "        ## 1. create a rank by sorting 'predictions'\n",
    "        ## Hint: use argsort()\n",
    "\n",
    "        rank =\n",
    "\n",
    "        ###################################################\n",
    "\n",
    "        valid_user += 1\n",
    "\n",
    "        if rank < 10:\n",
    "            NDCG += 1 / np.log2(rank + 2)\n",
    "            HR += 1\n",
    "        if valid_user % 100 == 0:\n",
    "            print('.', end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return NDCG / valid_user, HR / valid_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IK9z0wQIwzQ"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Now we will implement our SASRec model!\n",
    "\n",
    "*Note: evaluation will take quite a while without a GPU (~ 15 minutes)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f6QslzefPZ3"
   },
   "outputs": [],
   "source": [
    "# Please do not change the args\n",
    "args={\n",
    "    'datapath':'https://drive.google.com/uc?id=1sjeWz4pXkVGmy__Tr8zCtGV6rPGZfgLy',\n",
    "    'batch_size': 128,\n",
    "    'lr': 0.001,\n",
    "    'maxlen': 50,\n",
    "    'hidden_units': 50,\n",
    "    'num_blocks': 2,\n",
    "    'num_epochs': 101,\n",
    "    'num_heads': 1,\n",
    "    'dropout_rate': 0.5,\n",
    "    'l2_emb': 0.0,\n",
    "    'device': 'cpu',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 850895,
     "status": "ok",
     "timestamp": 1696495333846,
     "user": {
      "displayName": "Jane HAN",
      "userId": "01838841618913860328"
     },
     "user_tz": -540
    },
    "id": "XuPGKYuP7ot2",
    "outputId": "6e74595a-b656-436d-c1d7-d79a8df1632e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating........................................................................................................................epoch:20, time: 131.297342(s), valid (NDCG@10: 0.4714, HR@10: 0.7334), test (NDCG@10: 0.4487, HR@10: 0.7101)\n",
      "Evaluating........................................................................................................................epoch:40, time: 259.095778(s), valid (NDCG@10: 0.5333, HR@10: 0.7886), test (NDCG@10: 0.5060, HR@10: 0.7565)\n",
      "Evaluating........................................................................................................................epoch:60, time: 390.139411(s), valid (NDCG@10: 0.5524, HR@10: 0.8045), test (NDCG@10: 0.5275, HR@10: 0.7757)\n",
      "Evaluating........................................................................................................................epoch:80, time: 521.176454(s), valid (NDCG@10: 0.5629, HR@10: 0.8083), test (NDCG@10: 0.5349, HR@10: 0.7823)\n",
      "Evaluating........................................................................................................................epoch:100, time: 649.208588(s), valid (NDCG@10: 0.5672, HR@10: 0.8108), test (NDCG@10: 0.5403, HR@10: 0.7873)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # global dataset\n",
    "    dataset = data_partition(args['datapath'])\n",
    "\n",
    "    [user_train, user_valid, user_test, usernum, itemnum] = dataset\n",
    "    num_batch = len(user_train) // args['batch_size']\n",
    "\n",
    "    sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args['batch_size'], maxlen=args['maxlen'], n_workers=3)\n",
    "    model = SASRec(usernum, itemnum, args).to(args['device'])\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        try: torch.nn.init.xavier_normal_(param.data)\n",
    "        except: pass\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_start_idx = 1\n",
    "    bce_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    adam_optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], betas=(0.9, 0.98))\n",
    "\n",
    "    T = 0.0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(epoch_start_idx, args['num_epochs'] + 1):\n",
    "        for step in range(num_batch):\n",
    "        ##################Your code here ##################\n",
    "        ## Note: implement a training loop\n",
    "        ## 1. get u, seq, pos, neg from sampler.next_batch()\n",
    "        ##    (see how we defined the WarpSampler)\n",
    "        ## 2. convert these to np.array objects.\n",
    "        ## 3. predict pos_logits and neg_logits by running model() with u, seq, pos, neg\n",
    "\n",
    "            u, seq, pos, neg =\n",
    "            u, seq, pos, neg =\n",
    "            pos_logits, neg_logits =\n",
    "\n",
    "         ###################################################\n",
    "\n",
    "            pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args['device']), torch.zeros(neg_logits.shape, device=args['device'])\n",
    "            adam_optimizer.zero_grad()\n",
    "            indices = np.where(pos != 0)\n",
    "            loss = bce_criterion(pos_logits[indices], pos_labels[indices])\n",
    "            loss += bce_criterion(neg_logits[indices], neg_labels[indices])\n",
    "            for param in model.item_emb.parameters(): loss += args['l2_emb'] * torch.norm(param)\n",
    "            loss.backward()\n",
    "            adam_optimizer.step()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            t1 = time.time() - t0\n",
    "            T += t1\n",
    "            print('Evaluating', end='')\n",
    "            t_test = evaluate(model, dataset, True, args)\n",
    "            t_valid = evaluate(model, dataset, False, args)\n",
    "            print('epoch:%d, time: %f(s), valid (NDCG@10: %.4f, HR@10: %.4f), test (NDCG@10: %.4f, HR@10: %.4f)'\n",
    "                    % (epoch, T, t_valid[0], t_valid[1], t_test[0], t_test[1]))\n",
    "            t0 = time.time()\n",
    "            model.train()\n",
    "\n",
    "    sampler.close()\n",
    "    print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cms4486Bc3Ao"
   },
   "source": [
    "## Question 6: What are the test NDCG@10 and HR@10 for SASRec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aND1ZFdRAJuz"
   },
   "source": [
    "NDCG@10:\n",
    "\n",
    "HR@10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7JXsMTBgeOI"
   },
   "source": [
    "# Submission\n",
    "\n",
    "In order to get credit, you need to submit the `ipynb` file to LMS.\n",
    "\n",
    "To get this file, click `File` and `Download .ipynb`. Please make sure that your output of each cell is available in your `ipynb` file."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1VUP7iAhPsf0CNdyaTiK4gZLAETKEaIFl",
     "timestamp": 1695258082812
    },
    {
     "file_id": "1Aa0eKSmyYef1gORvlHv7EeQzSVRb30eL",
     "timestamp": 1623246611233
    },
    {
     "file_id": "1Jc5CAEGZIvY0vka3mBdf0tqn2TaJr2O1",
     "timestamp": 1610408674518
    },
    {
     "file_id": "1gc6u6hItUKY9uJt6GXHaneSYCMaGcxp1",
     "timestamp": 1610395347938
    },
    {
     "file_id": "1CqWY4pk7_VFxi8K8v4asr18ed0Hs8FVA",
     "timestamp": 1578441204356
    }
   ]
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
