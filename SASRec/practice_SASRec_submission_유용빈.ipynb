{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **AAI0026 Practice : SASRec**\n",
        "## **Self-Attentive Sequential Recommendation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j55lwZCol31I"
      },
      "source": [
        "<img width=\"500\" alt=\"1\" src=\"https://drive.google.com/uc?id=1En4xwSyUKfWVpRMfvJBgEXKHaIMLXvjt\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "In this Colab notebook, we will construct our own Sequential Recommendation model called Self-Attentive Sequential Recommendation (SASRec) by using PyTorch, and apply this model on the MovieLens dataset which is a widely used benchmark for evaluating collaborative filtering algorithms.\n",
        "\n",
        "\n",
        "Unlike Markov-Chain- (MC-), CNN- and RNN-based models, SASRec relies on Transformer encoder that generates a new representation of a sequence of items which a user interacted with. Experiments showed that SASRec outperforms the MC-based models and the CNN/RNN-based approaches.\n",
        "\n",
        "As shown in the figure above, (1) the item sequence goes through a block which consists of (2) Embedding Layer, (3)Self-Attention Layer (S-A layer), and (4) Point-Wise Feed Forward Network (P-W FFN). In this course, we stack two blocks, which are then followed by (5) Prediction Layer.\n",
        "\n",
        "We will go through these steps from loading input data to predicting user's next item.\n",
        "\n",
        "Have fun!\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGKqVEbbMEzf"
      },
      "source": [
        "# Device\n",
        "You might need to use GPU for this Colab.\n",
        "\n",
        "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0NiFL6OLpaJ"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "By2oyBw7Lrh5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import torch\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "from multiprocessing import Process, Queue\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5DED3P0us11s",
        "outputId": "e034c15c-2464-4a52-9359-82fcec1811eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwwq0nSdmsOL"
      },
      "source": [
        "# 1 Input Action Sequence\n",
        "- Load MovieLens Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf7vUmdNKCjA"
      },
      "source": [
        "MovieLens has several versions available. We will use the MovieLens 1M dataset, which has 1 million ratings from 6000 users on 4000 movies. Each line of the dataset file consists of a user ID, a movie ID, the user's rating on the movie, and a timestamp. However, given that SASRec is designed to learn from implicit information (interaction between the user and the movie), we will only utilize user IDs and movie IDs in our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmKQPpUaYFXc"
      },
      "source": [
        "## Train/Validation/Test Data Generation\n",
        "\n",
        "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=1G-S7FpUzlykFBNmVh-CtLUpmHh0-Y5XG\">\n",
        "</p>\n",
        "\n",
        "We will begin by loading the dataset into a Python dictionary with User ID = Key and Movie ID = Value. Then we will split the dataset to training/validation/test data. The second-last and the last Movie IDs from each user will be assigned for the validation and test data respectively while all the remaining will constitute the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vCmgBy-2VgJs"
      },
      "outputs": [],
      "source": [
        "def data_partition(data_path):\n",
        "   # TODO: Implement this function that takes a text file,\n",
        "   # convert the text file to the dictionary,\n",
        "   # and then split the dictionary to the training, validation, and test data\n",
        "\n",
        "    usernum = 0\n",
        "    itemnum = 0\n",
        "    User = defaultdict(list) # create a dictionary with default value as empty list\n",
        "    user_train = {}\n",
        "    user_valid = {}\n",
        "    user_test = {}\n",
        "\n",
        "    os.system(f'wget {data_path} -O data.txt')\n",
        "    with open('data.txt', 'r') as f:\n",
        "      for line in f:\n",
        "          u, i = map(int, line.rstrip().split(' '))\n",
        "          User[u].append(i)\n",
        "\n",
        "      ############# Your code here ############\n",
        "      ## Note:\n",
        "      ## 1. Compute the number 'usernum' of users, and\n",
        "      ##    the number 'itemnum' of items from the dictionary 'User'\n",
        "      ## 2. Each line of 'data.txt' consists of a user ID and an item ID\n",
        "      ## 3. A user ID starts from 1. User IDs are sorted in ascending order\n",
        "      ## 4. Hint:use max() function\n",
        "          usernum = max(u, usernum)\n",
        "          itemnum = max(i, itemnum)\n",
        "\n",
        "      #########################################\n",
        "\n",
        "    # The second-last and the last movies are for validation and test data respectively\n",
        "      for user in User:\n",
        "          nfeedback = len(User[user])\n",
        "          if nfeedback < 3:\n",
        "              user_train[user] = User[user]\n",
        "              user_valid[user] = []\n",
        "              user_test[user] = []\n",
        "          else:\n",
        "              user_train[user] = User[user][:-2]\n",
        "              user_valid[user] = []\n",
        "              user_valid[user].append(User[user][-2])\n",
        "              user_test[user] = []\n",
        "              user_test[user].append(User[user][-1])\n",
        "    return [user_train, user_valid, user_test, usernum, itemnum]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sCV3xJWCddX"
      },
      "source": [
        "## Question 1: What is the number of Users and Items?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1Np-FxA6tG1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cd23714-06e9-45c2-c9ad-03b979d0d92a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of users: 6040, number of items: 3416\n"
          ]
        }
      ],
      "source": [
        "data_path = 'https://drive.google.com/uc?id=1sjeWz4pXkVGmy__Tr8zCtGV6rPGZfgLy'\n",
        "user_train, user_valid, user_test, usernum, itemnum = data_partition(data_path)\n",
        "\n",
        "print(f'number of users: {usernum}, number of items: {itemnum}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqOcxa0vtFEA"
      },
      "source": [
        "## Question 2: What is the ID of the movie that User 26 has watched most recently?\n",
        "### (use dictionary user_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i_th = 26\n",
        "print('학습데이터: ',user_train[i_th])\n",
        "print('validation: ',user_valid[i_th])\n",
        "print('test: ',user_test[i_th])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfzNzh8xtG8T",
        "outputId": "f49d4eca-e2fa-4311-adfc-515239bca791"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습데이터:  [1100, 1135, 280, 242, 143, 862, 984, 47, 188, 693, 1703, 904, 4, 101, 527, 202, 1491, 1388, 680, 1704, 145, 150, 153, 278, 675, 255, 158, 903, 29, 161, 879, 673, 396, 27, 247, 159, 62, 1067, 176, 1074, 1705, 848, 809, 558, 813, 212, 586, 216, 1706, 1222, 1688, 817, 1707, 186, 853, 208, 949, 1136, 252, 1408, 1570, 139, 951, 877, 115, 1148, 1342, 1580, 410, 243, 1708, 1709, 241, 1, 752, 810, 368, 39, 808, 172, 203, 870, 180, 173, 804, 200, 209, 224, 389, 205, 191, 213, 123, 742, 971, 112, 126, 82, 802, 1710, 829, 1711, 893, 9, 234, 420, 461, 601, 1712, 653, 15, 297, 16, 254, 13, 149, 237, 244, 362, 1620, 613, 1106, 864, 1713, 20, 250, 1714, 681, 14, 22, 1102, 1427, 32, 24, 1419, 340, 717, 697, 866, 1616, 1715, 1716, 157, 167, 256, 641, 122, 818, 283, 179, 1331, 3, 134, 1104, 257, 109, 42, 624, 792, 1717, 52, 392, 800, 409, 795, 956, 1718, 827, 1719, 1720, 103, 636, 801, 1721, 466, 538, 44, 505, 46, 650, 842, 784, 707, 111, 498, 45, 2, 51, 775, 390, 175, 64, 1211, 843, 806, 76, 1722, 72, 100, 98, 590, 1133, 1481, 885, 705, 803, 857, 1723, 1134, 1520, 1724, 1725, 947, 1150, 1521, 194, 798, 704, 668, 550, 1726, 245, 1668, 229, 155, 614, 445, 777, 316, 1727, 230, 236, 756, 485, 206, 1171, 830, 585, 845, 591, 1560, 805, 1007, 820, 899, 765, 739, 959, 1561, 1242, 1476, 1728, 1475, 1080, 262, 1082, 1729, 1308, 43, 1175, 40, 163, 597, 327, 156, 1118, 574, 931, 874, 1730, 1179, 797, 1021, 69, 1575, 204, 1071, 1731, 1732, 865, 1130, 964, 61, 1697, 214, 113, 592, 1235, 679, 35, 222, 1535, 588, 854, 71, 725, 837, 1733, 642, 1734, 832, 967, 1037, 1735, 667, 1736, 1549, 1494, 957, 836, 1737, 1046, 1738, 855, 1739, 1507, 1405, 1144, 1455, 1355, 116, 1740, 1741, 1458, 1638, 1742, 1396, 1743, 476, 900, 418, 533, 1127, 282, 1122, 1744, 1649, 301, 451, 480, 638, 504, 439, 96, 782, 887, 915, 1745, 1746, 1063, 130, 942, 1338, 950, 1747, 1418, 10, 902, 861, 1748, 1749, 1349, 1750, 646, 1687, 748, 1119, 1751, 762, 1752, 1753, 440, 662, 1754, 736, 1755, 1756, 373, 1757, 1758, 41, 652, 880, 154, 1759, 778, 763, 737, 833, 226, 6, 37, 68, 757, 733, 630, 185, 595, 108, 199, 1760, 1017, 684, 607, 518, 197, 94]\n",
            "validation:  [239]\n",
            "test:  [901]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ATGeg8jWt8VP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9af162-fde7-44a8-c89a-a8ada11db57a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The last movie that User 26 has watched: [901]\n"
          ]
        }
      ],
      "source": [
        "print(f'The last movie that User 26 has watched: {user_test[26]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NHYKpHeYJHm"
      },
      "source": [
        "## Sampler for Batch Generation\n",
        "\n",
        "\n",
        "<img width=\"800\" alt=\"1\" src=\"https://drive.google.com/uc?id=1cbB_RfDtPN3kGOcO_FDEz3YbXt6y1a39\">\n",
        "</p>\n",
        "\n",
        "As each user has interacted with a different number of movies, we should transform a training sequence into the fixed-length sequence with the maximum length (maxlen=200). If the sequence length is greater than 200, we retain only the most recent 200 movies. If the sequence length is less than 200, we repeatedly add a padding item (0) to the left until the length becomes 200. A constant zero vector is used as the embedding for the padding item.\n",
        "\n",
        "- seq: Input NumPy ndarray (batch_size*maxlen) for Embedding Layer.\n",
        "- pos: Ground Truth NumPy ndarray (batch_size*maxlen), the first movie is deleted and the last movie is added as our task is to predict the next item.\n",
        "- neg: Negative Movie Lists Numpy ndarray (batch_size*maxlen), randomly sampled from the movies that have interacted with no users.\n",
        "\n",
        "NumPy ndarrays seg, pos, and neg have the same shape, and pos and neg will be used when computing loss at Prediction Layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F_vhRYIXYbmy"
      },
      "outputs": [],
      "source": [
        "#Negative Sampling: select items not in the set of positive items\n",
        "def random_neq(l, r, s):\n",
        "    ##TODO: Implement this function that takes two indices 'l', 'r', and set 's',\n",
        "    ##and return a random integer number t in the range [l,r]\n",
        "    ##but not in set 's' so it can generate a new random number\n",
        "\n",
        "    t = np.random.randint(l, r)\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## Note:\n",
        "    ## 1. to ensure that t is not in set 's', use while loop\n",
        "    ## 2. repeat generating random numbers until t is not in set 's'\n",
        "    ## 3. (~3 lines of code)\n",
        "    while t in s:\n",
        "        t = np.random.randint(l, r)\n",
        "    return t\n",
        "\n",
        "    #########################################\n",
        "\n",
        "#generate seg, pos, neg numpy ndarrays.\n",
        "#they are the fixed-length sequences ('maxlen')\n",
        "\n",
        "def sample_function(user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED):\n",
        "    def sample():\n",
        "    ##TODO: Implement this function that takes arguments,\n",
        "    ##and return numpy ndarrays user, seq, pos and neg\n",
        "\n",
        "        user = np.random.randint(1, usernum + 1)\n",
        "        while len(user_train[user]) <= 1: user = np.random.randint(1, usernum + 1)\n",
        "\n",
        "        seq = np.zeros([maxlen], dtype=np.int32) #zeros for padding\n",
        "        pos = np.zeros([maxlen], dtype=np.int32)\n",
        "        neg = np.zeros([maxlen], dtype=np.int32)\n",
        "        nxt = user_train[user][-1]\n",
        "        idx = maxlen - 1\n",
        "        # print(maxlen)\n",
        "        ts = set(user_train[user]) #positive item set\n",
        "        for i in reversed(user_train[user][:-1]): #reverse for padding from the left\n",
        "\n",
        "            seq[idx] = i\n",
        "            pos[idx] = nxt\n",
        "\n",
        "            if nxt != 0:\n",
        "            ############# Your code here ############\n",
        "            ##Note:\n",
        "            ##1. generate neg Numpy ndarray by using random_neg(l,r,s)\n",
        "            ##2. fill in neg[idx] by using the for loop we are in\n",
        "            ##3. remember item ID starts from 1\n",
        "            ##4. (~1 line of code)\n",
        "                neg[idx] = random_neq(1, itemnum+1, ts) # item 1 ~ 3416\n",
        "\n",
        "            #########################################\n",
        "            nxt = i\n",
        "            idx -= 1\n",
        "            if idx == -1: break\n",
        "\n",
        "        return (user, seq, pos, neg)\n",
        "\n",
        "    # Make sure random numbers generated are always the same everytime\n",
        "    np.random.seed(SEED)\n",
        "\n",
        "    while True:\n",
        "        one_batch = []\n",
        "        for i in range(batch_size):\n",
        "            one_batch.append(sample())\n",
        "        result_queue.put(zip(*one_batch))\n",
        "\n",
        "class WarpSampler(object):\n",
        "    def __init__(self, User, usernum, itemnum, batch_size=64, maxlen=10, n_workers=1):\n",
        "    ##TODO: Implement this function that takes arguments,\n",
        "    ##and process sampling in parallel using multiple worker processes\n",
        "\n",
        "        self.result_queue = Queue(maxsize=n_workers*10)\n",
        "        self.processors = []\n",
        "\n",
        "        for i in range(n_workers):\n",
        "\n",
        "            ############# Your code here ###################################\n",
        "            ##Note:\n",
        "            ##1. append \"Process(target, args)\" object in self.processors\n",
        "            ##2. set target to \"sample_function\"\n",
        "            ##3. create a tuple of all arguemnts of sample_function,\n",
        "            ##.  result_queue='self.result_queue', and SEED='np.random.randint(2e9)'.\n",
        "            ##4. set args to this tuple\n",
        "            self.processors.append(\n",
        "                Process(target=sample_function, args=(User,usernum,itemnum,batch_size,maxlen,self.result_queue,np.random.randint(2e9)))\n",
        "            )\n",
        "\n",
        "            #################################################################\n",
        "\n",
        "            self.processors[-1].daemon = True #doesn't prevent the program from exiting\n",
        "            self.processors[-1].start() #To begine the Multiprocess\n",
        "\n",
        "    #get result_queue when it becomes available, without waiting for all processes to finish\n",
        "    def next_batch(self):\n",
        "        return self.result_queue.get()\n",
        "\n",
        "    def close(self):\n",
        "        for p in self.processors:\n",
        "            p.terminate()\n",
        "            p.join()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKhcVeAhCwoY"
      },
      "source": [
        "## Question 3: Does our sampler work well for User 26?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPWl0hDO341V",
        "outputId": "09669c10-cf87-4d37-91dd-ee11eb90b960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Works Well!\n"
          ]
        }
      ],
      "source": [
        "sampler = WarpSampler(user_train, usernum, itemnum, batch_size=64, maxlen=10, n_workers=3)\n",
        "\n",
        "for _ in range(1):  # Check one batch\n",
        "    u, seq, pos, neg = sampler.next_batch()\n",
        "\n",
        "    ############# Your code here ###################################\n",
        "    ##note:\n",
        "    ##1.use any() function\n",
        "    ##2.check if neg[26] items are in pos[26]\n",
        "\n",
        "    same_item = any([i in pos[26] for i in neg[26]])\n",
        "\n",
        "    #################################################################\n",
        "    if same_item==False:\n",
        "      print(\"Works Well!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Uhrl6Mqsp2"
      },
      "source": [
        "# 2 SASRec Model\n",
        "\n",
        "Now we will implement our SASRec model!\n",
        "\n",
        "Please see the following:\n",
        "\n",
        "1.   Embedding Layer (Item Embedding, Position Embedding):\n",
        "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=16w1sfleDGnnKEQvL-wwPO4REwt3te1TA\">\n",
        "</p>\n",
        "\n",
        "  We will obtain an `item embedding` and a learnable `position embedding` by using this layer. Every item ID in `seq` turns into its item embedding, which gives Tensor(M) of `item embeddings` in shape batch_size(128) x max_len(200) x d(50). Since the self-attention model doesn't include any recurrent or convolutional module, it's impossible to be aware of the positions of previous items. Hence we inject a `position embedding` that represents the position of value in `seq`, i.e., max_len (200) positions ranging 0~199. The positions are encoded to Tensor(P) with the same shape as M. Final `input embedding(E=M+P)` serves as Key and Value, and the layer normalization of input embedding serves as Query.\n",
        "\n",
        "2.   Self-Attention Layer:\n",
        "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=1UuTRCQuCQjf3p49wxPO4z3_95xGE0SqR\">\n",
        "</p>\n",
        "\n",
        "     - Feed (Q,K,V) into Attention layer and compute `attention scores(S)`\n",
        "     - when proceeding dot production of Query and Key (Q*K), need `Attention Masking` to avoid cheating\n",
        "     - Residual Connection\n",
        "     - Layer Normalization\n",
        "\n",
        "3.   Pointwise Feed Forward Network:\n",
        "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=1bij9ohJtxj4UhpB9OcjRnK8rw9bQLqVz\">\n",
        "</p>\n",
        "\n",
        "  Feed Forward Network consists of \"two 1d Convolution Layers\" which functions as linear transformation. Before passing 1d-Convolution Layer, input data need to be transposed to allow the kernel to move across time-steps. These Convolution Layers are linked with `ReLU activation` and their outputs will be added with the input data for Residual Connection. After Forward Layer the masking (timeline mask), which was used in item embedding, is applied to preserve their initial padding positions.    \n",
        "\n",
        "4.   Prediction Layer:\n",
        "<img width=\"700\" alt=\"1\" src=\"https://drive.google.com/uc?id=14ACxSsuyxeKdtA8QG2HN_XGzV0-p_xSc\">\n",
        "</p>\n",
        "\n",
        "  Finally, Prediction Layer generates `pos_logits` and `neg_logits`, i.e., the scores of movies in positive sets and negative sets respectively for users.\n",
        "  We compute the \"element-wise product\" between pos embedding and attention scores (`log_feats`) obtained in Attention Layer to generate `pos_logits`. We also compute the element-wise product between neg_embedding and the attention score to generate `neg_logits`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQqKUwqU5Qf8"
      },
      "source": [
        "## (1) Build Layers for SASRec\n",
        "- Embedding Layer\n",
        "- Self-Attention Layer\n",
        "- Forward Layer\n",
        "- Prediction Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WEsLOTQkpsUC"
      },
      "outputs": [],
      "source": [
        "class SASRec(torch.nn.Module):\n",
        "    def __init__(self, user_num, item_num, args):\n",
        "        super(SASRec, self).__init__()\n",
        "\n",
        "        ##TODO: stack embedding layer, attention layer and forward layer,\n",
        "        ##define the function that computes Attention Scores(log_feat)\n",
        "        ##and get pos_logits, neg_logits from FFN and Prediction\n",
        "\n",
        "        self.user_num = user_num\n",
        "        self.item_num = item_num\n",
        "        self.dev = args['device']\n",
        "\n",
        "        self.item_emb = torch.nn.Embedding(self.item_num+1, args['hidden_units'], padding_idx=0)\n",
        "\n",
        "        ##################Your code here ##################\n",
        "        ## Note:\n",
        "        ## 1. Define position embedding and dropout\n",
        "        ## 2. See how item embedding is created above\n",
        "        ## 3. use arg['maxlen'], args['hidden_units'], args['dropout_rate']\n",
        "        ## 4. position embedding doesn't need padding idx\n",
        "\n",
        "        self.pos_emb = torch.nn.Embedding(args['maxlen'], args['hidden_units']) # learnable pos embedding\n",
        "        self.emb_dropout = torch.nn.Dropout(args['dropout_rate'])\n",
        "\n",
        "        ###################################################\n",
        "\n",
        "        self.attention_layernorms = torch.nn.ModuleList() # to be Query for self-attention\n",
        "        self.attention_layers = torch.nn.ModuleList() #multi-head for self-attention\n",
        "        self.forward_layernorms = torch.nn.ModuleList()\n",
        "        self.forward_layers = torch.nn.ModuleList()\n",
        "        self.last_layernorm = torch.nn.LayerNorm(args['hidden_units'], eps=1e-8)\n",
        "\n",
        "        for _ in range(args['num_blocks']): #stacks 2 blocks\n",
        "\n",
        "            new_attn_layernorm = torch.nn.LayerNorm(args['hidden_units'], eps=1e-8)\n",
        "            self.attention_layernorms.append(new_attn_layernorm)\n",
        "\n",
        "            # 멀티헤드 어텐션 args : embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, batch_first=False, device=None, dtype=None\n",
        "            new_attn_layer =  torch.nn.MultiheadAttention(args['hidden_units'],\n",
        "                                                            args['num_heads'],\n",
        "                                                            args['dropout_rate'])\n",
        "            self.attention_layers.append(new_attn_layer)\n",
        "\n",
        "            new_fwd_layernorm = torch.nn.LayerNorm(args['hidden_units'], eps=1e-8)\n",
        "            self.forward_layernorms.append(new_fwd_layernorm)\n",
        "\n",
        "            new_fwd_layer = PointWiseFeedForward(args['hidden_units'], args['dropout_rate'])\n",
        "            self.forward_layers.append(new_fwd_layer)\n",
        "\n",
        "            self.pos_sigmoid = torch.nn.Sigmoid()\n",
        "            self.neg_sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def log2feats(self, log_seqs):\n",
        "        ## TODO: Implement this function that takes log_seqs,\n",
        "        ## and make input tensor(item embedding + position embedding) for self-attention layer\n",
        "        ## then return attention scores(log_feats) tensor from self-attention layer\n",
        "        # print(\"[1]:\",log_seqs.shape)\n",
        "        # print(log_seqs)\n",
        "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.dev))\n",
        "        # print(\"[2]:\",seqs.shape)\n",
        "        seqs *= self.item_emb.embedding_dim ** 0.5 #scaling to stabilize the training process\n",
        "\n",
        "        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1])\n",
        "\n",
        "        ##################Your code here ##################\n",
        "        ## Note:\n",
        "        ## 1. Sum position embedding to input embedding(seqs)\n",
        "        ## 2. Proceed the dropout after\n",
        "        ## 3. (~2 lines of code)\n",
        "        seqs += self.pos_emb(torch.LongTensor(positions).to(self.dev))\n",
        "        seqs = self.emb_dropout(seqs)\n",
        "\n",
        "        ###################################################\n",
        "\n",
        "        timeline_mask = torch.BoolTensor(log_seqs == 0).to(self.dev)\n",
        "        seqs *= ~timeline_mask.unsqueeze(-1) # broadcast in last dim\n",
        "\n",
        "        tl = seqs.shape[1] # time dim len for enforce causality\n",
        "        attention_mask = ~torch.tril(torch.ones((tl, tl), dtype=torch.bool, device=self.dev))\n",
        "        #to mask upper triangular part\n",
        "\n",
        "        for i in range(len(self.attention_layers)):\n",
        "            seqs = torch.transpose(seqs, 0, 1)\n",
        "\n",
        "           ##################Your code here ##################\n",
        "           ## Note:\n",
        "           ## 1. get Q by attention_layernorms[]()\n",
        "           ## 2. get multihead attention outputs (mha_outputs): sum of the weighted V\n",
        "           ##    by attention_layers[]() with using attention_mask\n",
        "           ## 3. key, value = seqs\n",
        "\n",
        "            Q = self.attention_layernorms[i](seqs)\n",
        "            mha_outputs, _ = self.attention_layers[i](Q, seqs, seqs, attn_mask = attention_mask)\n",
        "\n",
        "            ###################################################\n",
        "            seqs = Q + mha_outputs\n",
        "            seqs = torch.transpose(seqs, 0, 1)\n",
        "\n",
        "            seqs = self.forward_layernorms[i](seqs)\n",
        "            seqs = self.forward_layers[i](seqs)\n",
        "            seqs *=  ~timeline_mask.unsqueeze(-1)\n",
        "\n",
        "        log_feats = self.last_layernorm(seqs)\n",
        "\n",
        "        return log_feats #Attention Scores\n",
        "\n",
        "    def forward(self, user_ids, log_seqs, pos_seqs, neg_seqs):\n",
        "        log_feats = self.log2feats(log_seqs)\n",
        "\n",
        "        pos_embs = self.item_emb(torch.LongTensor(pos_seqs).to(self.dev))\n",
        "        neg_embs = self.item_emb(torch.LongTensor(neg_seqs).to(self.dev))\n",
        "\n",
        "        ##################Your code here ##################\n",
        "        ## Note: get pos_logits and neg_logits\n",
        "        ## 1.compute Attention Scores * Value(pos_embs/neg_embs)\n",
        "        ## Hint: use sum(dim=-1)\n",
        "        ## (~2 lines of code)\n",
        "\n",
        "        pos_logits = (log_feats * pos_embs).sum(dim=-1)\n",
        "        neg_logits = (log_feats * neg_embs).sum(dim=-1)\n",
        "\n",
        "        ###################################################\n",
        "\n",
        "        pos_pred = self.pos_sigmoid(pos_logits)\n",
        "        neg_pred = self.neg_sigmoid(neg_logits)\n",
        "\n",
        "        return pos_logits, neg_logits\n",
        "\n",
        "    def predict(self, user_ids, log_seqs, item_indices):\n",
        "        log_feats = self.log2feats(log_seqs)\n",
        "\n",
        "        final_feat = log_feats[:, -1, :] # the final Attention Scores for the prediction\n",
        "\n",
        "        item_embs = self.item_emb(torch.LongTensor(item_indices).to(self.dev))\n",
        "\n",
        "        logits = item_embs.matmul(final_feat.unsqueeze(-1)).squeeze(-1) #dot product between item's embedding and final feature\n",
        "                                                                        #squeeze to make dimensions match\n",
        "        preds = self.pos_sigmoid(logits)\n",
        "\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUu28G0ihX2J"
      },
      "source": [
        "## Question 4: What does the output (preds) of SASRec mean?\n",
        "\n",
        "(1) list of next items to recommend for the users\n",
        "\n",
        "(2) Probability of each items to recommend for the users\n",
        "\n",
        "(3) Yes/No on each items to recomeend for the users\n",
        "\n",
        "(4) Tensor of next items to recommend for the users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIdqVTjowZiR"
      },
      "source": [
        "Answer: 2 (유저에게 추천할 각 아이템들의 확률분포)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4CcOUEoInjD"
      },
      "source": [
        "## (2) Pointwise Feed Forward Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-DCtgcHpGIpd"
      },
      "outputs": [],
      "source": [
        "class PointWiseFeedForward(torch.nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "\n",
        "        super(PointWiseFeedForward, self).__init__()\n",
        "\n",
        "        ##################Your code here ##################\n",
        "        ## Note:\n",
        "        ## 1. stack 2 Convolution Layers\n",
        "        ## 2. after each convolution layer, proceed dropout\n",
        "        ## 3. add relu after the first dropout and before the second convolution\n",
        "        ## Hint: use torch.nn.Conv1d with kernel_size=1\n",
        "\n",
        "        self.conv1 = torch.nn.Conv1d(hidden_units, hidden_units, kernel_size=1)\n",
        "        self.dropout1 = torch.nn.Dropout(p=dropout_rate)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.conv2 = torch.nn.Conv1d(hidden_units, hidden_units, kernel_size=1)\n",
        "        self.dropout2 = torch.nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        ###################################################\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.dropout2(self.conv2(self.relu(self.dropout1(self.conv1(inputs.transpose(-1, -2)))))) #transpose so kernel can pass by time steps\n",
        "        outputs = outputs.transpose(-1, -2) # return it back\n",
        "        outputs += inputs #Residual Connection\n",
        "\n",
        "        # print(\"final output shape : \",outputs.shape)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duMEg-olLjbJ"
      },
      "source": [
        "## Question 5: The shape of output for the two convolution layer becomes batch_size(128) x d(50) x max_len(200). What will be the shape of the final output of the forward function after transpose and residual connection?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbiOGL3QlFMu"
      },
      "source": [
        "Shape of final outputs: [128, 200, 50] # 밑에 args 에 파라미터와 달라, max len 200 으로 통일하였습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUA815bNJ8w"
      },
      "source": [
        "# 3 Evaluate on Test/Validation data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g-8-kuVbepAS"
      },
      "outputs": [],
      "source": [
        "#randomly sample 100 negative items and rank these items with the ground truth item. Based on the rankings we can evaluate Hit@10 and NDCG@10\n",
        "def evaluate(model, dataset, selT, args):\n",
        "    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)\n",
        "\n",
        "    NDCG = 0.0 #Normalized Discounted Cumulative Gain: evaluates the ranked quality of recommendations\n",
        "               #by considering both position and relevance of the ground truth item in the recommendation list\n",
        "    HR = 0.0 #Hit Rate@k: measures the fraction of times the ground truth next item is among the top-k recommendations\n",
        "    valid_user = 0.0\n",
        "\n",
        "    if selT==True:\n",
        "      VT=test\n",
        "    else:\n",
        "      VT=valid\n",
        "\n",
        "    users = range(1,usernum+1)\n",
        "    for u in users:\n",
        "        if len(train[u]) < 1 or len(VT[u]) < 1: continue\n",
        "\n",
        "        seq = np.zeros([args['maxlen']], dtype=np.int32)\n",
        "        idx = args['maxlen'] - 1\n",
        "        if selT == True:\n",
        "          seq[idx] = valid[u][0]\n",
        "          idx -= 1\n",
        "\n",
        "        for i in reversed(train[u]):\n",
        "            seq[idx] = i\n",
        "            idx -= 1\n",
        "            if idx == -1: break\n",
        "\n",
        "        rated = set(train[u])\n",
        "        rated.add(0)\n",
        "        item_idx = [VT[u][0]]\n",
        "\n",
        "        for _ in range(100):\n",
        "            t = np.random.randint(1, itemnum + 1)\n",
        "            while t in rated: t = np.random.randint(1, itemnum + 1)\n",
        "            item_idx.append(t)\n",
        "\n",
        "        predictions = -model.predict(*[np.array(l) for l in [[u], [seq], item_idx]])\n",
        "        predictions = predictions[0]\n",
        "\n",
        "        ##################Your code here ##################\n",
        "        ## Note:\n",
        "        ## 1. create a rank by sorting 'predictions'\n",
        "        ## Hint: use argsort()\n",
        "\n",
        "        rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "        ###################################################\n",
        "\n",
        "        valid_user += 1\n",
        "\n",
        "        if rank < 10:\n",
        "            NDCG += 1 / np.log2(rank + 2)\n",
        "            HR += 1\n",
        "        if valid_user % 100 == 0:\n",
        "            print('.', end=\"\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    return NDCG / valid_user, HR / valid_user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IK9z0wQIwzQ"
      },
      "source": [
        "## Prediction\n",
        "\n",
        "Now we will implement our SASRec model!\n",
        "\n",
        "*Note: evaluation will take quite a while without a GPU (~ 15 minutes)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0f6QslzefPZ3"
      },
      "outputs": [],
      "source": [
        "# Please do not change the args\n",
        "args={\n",
        "    'datapath':'https://drive.google.com/uc?id=1sjeWz4pXkVGmy__Tr8zCtGV6rPGZfgLy',\n",
        "    'batch_size': 128,\n",
        "    'lr': 0.001,\n",
        "    'maxlen': 200, # 문제와 동일하게 200으로 수정함.\n",
        "    'hidden_units': 50,\n",
        "    'num_blocks': 2,\n",
        "    'num_epochs': 101,\n",
        "    'num_heads': 1,\n",
        "    'dropout_rate': 0.5,\n",
        "    'l2_emb': 0.0,\n",
        "    'device': 'cpu',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuPGKYuP7ot2",
        "outputId": "b21df6d2-48c1-4afa-e436-d635710a125d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating........................................................................................................................epoch:20, time: 683.676578(s), valid (NDCG@10: 0.4830, HR@10: 0.7492), test (NDCG@10: 0.4638, HR@10: 0.7250)\n",
            "Evaluating........................................................................................................................epoch:40, time: 1365.203602(s), valid (NDCG@10: 0.5505, HR@10: 0.8013), test (NDCG@10: 0.5232, HR@10: 0.7791)\n",
            "Evaluating........................................................................................................................epoch:60, time: 2035.611916(s), valid (NDCG@10: 0.5690, HR@10: 0.8118), test (NDCG@10: 0.5465, HR@10: 0.7919)\n",
            "Evaluating........................................................................................................................epoch:80, time: 2704.991881(s), valid (NDCG@10: 0.5797, HR@10: 0.8262), test (NDCG@10: 0.5587, HR@10: 0.7995)\n",
            "Evaluating........................................................................................................................epoch:100, time: 3372.488433(s), valid (NDCG@10: 0.5888, HR@10: 0.8305), test (NDCG@10: 0.5655, HR@10: 0.8061)\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    # global dataset\n",
        "    dataset = data_partition(args['datapath'])\n",
        "\n",
        "    [user_train, user_valid, user_test, usernum, itemnum] = dataset\n",
        "    # print(usernum, itemnum)\n",
        "    num_batch = len(user_train) // args['batch_size']\n",
        "    # print(num_batch)\n",
        "\n",
        "    sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args['batch_size'], maxlen=args['maxlen'], n_workers=3)\n",
        "    model = SASRec(usernum, itemnum, args).to(args['device'])\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        try: torch.nn.init.xavier_normal_(param.data)\n",
        "        except: pass\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_start_idx = 1\n",
        "    bce_criterion = torch.nn.BCEWithLogitsLoss()\n",
        "    adam_optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], betas=(0.9, 0.98))\n",
        "\n",
        "    T = 0.0\n",
        "    t0 = time.time()\n",
        "\n",
        "    for epoch in range(epoch_start_idx, args['num_epochs'] + 1):\n",
        "        for step in range(num_batch):\n",
        "        ##################Your code here ##################\n",
        "        ## Note: implement a training loop\n",
        "        ## 1. get u, seq, pos, neg from sampler.next_batch()\n",
        "        ##    (see how we defined the WarpSampler)\n",
        "        ## 2. convert these to np.array objects.\n",
        "        ## 3. predict pos_logits and neg_logits by running model() with u, seq, pos, neg\n",
        "\n",
        "            u, seq, pos, neg = sampler.next_batch()\n",
        "            u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)\n",
        "            # print(\"[0]:\",seq.shape)\n",
        "            pos_logits, neg_logits = model(u, seq, pos, neg)\n",
        "\n",
        "         ###################################################\n",
        "\n",
        "            pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args['device']), torch.zeros(neg_logits.shape, device=args['device'])\n",
        "            adam_optimizer.zero_grad()\n",
        "            indices = np.where(pos != 0)\n",
        "            loss = bce_criterion(pos_logits[indices], pos_labels[indices])\n",
        "            loss += bce_criterion(neg_logits[indices], neg_labels[indices])\n",
        "            for param in model.item_emb.parameters(): loss += args['l2_emb'] * torch.norm(param)\n",
        "            loss.backward()\n",
        "            adam_optimizer.step()\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            model.eval()\n",
        "            t1 = time.time() - t0\n",
        "            T += t1\n",
        "            print('Evaluating', end='')\n",
        "            t_test = evaluate(model, dataset, True, args)\n",
        "            t_valid = evaluate(model, dataset, False, args)\n",
        "            print('epoch:%d, time: %f(s), valid (NDCG@10: %.4f, HR@10: %.4f), test (NDCG@10: %.4f, HR@10: %.4f)'\n",
        "                    % (epoch, T, t_valid[0], t_valid[1], t_test[0], t_test[1]))\n",
        "            t0 = time.time()\n",
        "            model.train()\n",
        "\n",
        "    sampler.close()\n",
        "    print(\"Done\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cms4486Bc3Ao"
      },
      "source": [
        "## Question 6: What are the test NDCG@10 and HR@10 for SASRec?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aND1ZFdRAJuz"
      },
      "source": [
        "NDCG@10: 0.5655\n",
        "\n",
        "HR@10: 0.8061"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JXsMTBgeOI"
      },
      "source": [
        "# Submission\n",
        "\n",
        "In order to get credit, you need to submit the `ipynb` file to LMS.\n",
        "\n",
        "To get this file, click `File` and `Download .ipynb`. Please make sure that your output of each cell is available in your `ipynb` file."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}